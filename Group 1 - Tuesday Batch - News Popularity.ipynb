{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1>Team 1 - Tuesday Batch</h1> </center>\n",
    "<center> <h3>Aditi Chawla, Aditya Chauhan, Niti Saluja, Zaid Khan</h3> </center>\n",
    "<center> <h1>Breaking the Social Media </h1> </center>\n",
    "<center> <h1>News Popularity Prediction</h1> </center>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<center> <h3>IST 718 – Big Data Analytics</h3> </center>\n",
    "<center> <h3>Daniel E. Acuna</h3> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem & Objectives\n",
    "\n",
    "- In today’s world most of the people get the current information of current happenings from online news and articles. The current market of online news is large and is growing faster than expected .This leads to tough competition between online publishers and social media platforms in order to reach the largest possible audience.\n",
    "\n",
    "- One of the important pre-requisites in this industry is to have a highly efficient aligned online strategy based on the trustworthy predictionsof how popular these contents are on different platforms. Given the analysis of the current data we aim at predicting the future popularity of online news given recently published contents ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set Description\n",
    "\n",
    "- The dataset was downloaded from UCI Machine Learning repository under the link https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms# \n",
    "    - The dataset can be downloaded from the \"Data Folder\" option or by accessing the following link:\n",
    "    https://archive.ics.uci.edu/ml/machine-learning-databases/00432/Data/\n",
    "    - The dataset being used is \"News_Final.csv\"\n",
    "<br>\n",
    "- The dataset consists of 12 major columns and has about 93239 records. \n",
    "- It consists of data ranging from November 2015 to July 2016 and across various social media platforms- Google plus , Facebook and LinkedIn .It also contains the data about Sentiment Tittle , Topic it was published on before populating on these social media platforms along with publish date. Other columns are sparsely filled and roam around source it was published and Title of article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Dictionary\n",
    "\n",
    "- IDLink (numeric): Unique identifier of news items \n",
    "- Title (string): Title of the news item according to the official media sources \n",
    "- Headline (string): Headline of the news item according to the official media sources \n",
    "- Source (string): Original news outlet that published the news item \n",
    "- Topic (string): Query topic used to obtain the items in the official media sources \n",
    "- PublishDate (timestamp): Date and time of the news items' publication \n",
    "- SentimentTitle (numeric): Sentiment score of the text in the news items' title \n",
    "- SentimentHeadline (numeric): Sentiment score of the text in the news items' headline \n",
    "- Facebook (numeric): Final value of the news items' popularity according to the social media source Facebook \n",
    "- GooglePlus (numeric): Final value of the news items' popularity according to the social media source Google+ \n",
    "- LinkedIn (numeric): Final value of the news items' popularity according to the social media source LinkedIn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages Used\n",
    "\n",
    "- from pyspark.sql import **SparkSession**\n",
    "- from pyspark.ml import **feature, regression, evaluation, Pipeline**\n",
    "- from pyspark.sql import **functions** as fn, **Row**\n",
    "- import **matplotlib.pyplot** as plt\n",
    "- from pyspark.ml import **feature**\n",
    "- from pyspark.ml import **regression**\n",
    "- from pyspark.ml import **classification**\n",
    "- from pyspark.ml import **Pipeline**\n",
    "- import **pandas** as pd\n",
    "- import **numpy as** np\n",
    "- import **seaborn** as sns\n",
    "- from pyspark.ml.feature import **MaxAbsScaler**\n",
    "- from pyspark.ml.feature import **Tokenizer**\n",
    "- from pyspark.ml.feature import **CountVectorizer**\n",
    "- from pyspark.ml.feature import **VectorAssembler**\n",
    "- from pyspark.ml.feature import **IDF**\n",
    "- from pyspark.ml.classification import **LogisticRegression**\n",
    "- from pyspark.ml.classification import **RandomForestClassifier**\n",
    "- from pyspark.ml.feature import **IndexToString, StringIndexer, VectorIndexer**\n",
    "- from pyspark.ml.regression import **RandomForestRegressor**\n",
    "- from pyspark.ml.feature import **VectorIndexer**\n",
    "- from pyspark.ml.evaluation import **RegressionEvaluator**\n",
    "- from pyspark.ml.regression import **GBTRegressor**\n",
    "- from pyspark.ml.feature import **NGram**\n",
    "- from pyspark.sql.functions import **col, size**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poster\n",
    "<br>\n",
    "<center> <img src=\"Poster.png\" width=\"100%\" align=\"center\"> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import feature, regression, evaluation, Pipeline\n",
    "from pyspark.sql import functions as fn, Row\n",
    "import matplotlib.pyplot as plt\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functionality for computing features\n",
    "from pyspark.ml import feature\n",
    "# Functionality for regression\n",
    "from pyspark.ml import regression\n",
    "# Funcionality for classification\n",
    "from pyspark.ml import classification\n",
    "# Object for creating sequences of transformations\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql.functions import col, size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "datasource = pd.read_csv('News_Final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93239"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Omitting the invalid values\n",
    "- As the dataset contains -1 values for the popularity score, we shall be omitting those values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datasource = datasource[datasource['LinkedIn'] != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a User Defined Generic function to check the null values in each column of a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_table(df):\n",
    "        mis_val = df.isnull().sum()\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your selected dataframe has 11 columns.\n",
      "There are 2 columns that have missing values.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>% of Total Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Source</th>\n",
       "      <td>223</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Headline</th>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Missing Values  % of Total Values\n",
       "Source               223                0.3\n",
       "Headline              14                0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values_table(datasource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDLink</th>\n",
       "      <th>Title</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Source</th>\n",
       "      <th>Topic</th>\n",
       "      <th>PublishDate</th>\n",
       "      <th>SentimentTitle</th>\n",
       "      <th>SentimentHeadline</th>\n",
       "      <th>Facebook</th>\n",
       "      <th>GooglePlus</th>\n",
       "      <th>LinkedIn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>80690.0</td>\n",
       "      <td>Monday, 29 Feb 2016</td>\n",
       "      <td>RAMALLAH, February 25, 2016 (WAFA) - Palestine...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>palestine</td>\n",
       "      <td>2016-02-28 14:03:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.005906</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>81052.0</td>\n",
       "      <td>Monday, 29 Feb 2016</td>\n",
       "      <td>RAMALLAH, February 29, 2016 (WAFA) - The Gover...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>palestine</td>\n",
       "      <td>2016-03-01 09:29:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048546</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>80994.0</td>\n",
       "      <td>Tuesday, 1 Mar 2016</td>\n",
       "      <td>RAMALLAH, February 29, 2016 (WAFA) - The Gover...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>palestine</td>\n",
       "      <td>2016-03-01 00:15:00</td>\n",
       "      <td>-0.243068</td>\n",
       "      <td>0.048546</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>311.0</td>\n",
       "      <td>Microsoft offering £100 discount on original B...</td>\n",
       "      <td>Just ahead of the market rollout of Microsoft'...</td>\n",
       "      <td>International Business Times via Yahoo UK &amp; Ir...</td>\n",
       "      <td>microsoft</td>\n",
       "      <td>2015-11-08 05:25:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.132812</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>252.0</td>\n",
       "      <td>Economy to improve though no change in last 6 ...</td>\n",
       "      <td>&amp;quot;In the coming six months, there seems to...</td>\n",
       "      <td>IANS India Private Limited/Yahoo India News vi...</td>\n",
       "      <td>economy</td>\n",
       "      <td>2015-11-08 05:40:00</td>\n",
       "      <td>-0.082022</td>\n",
       "      <td>0.205537</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      IDLink                                              Title  \\\n",
       "6    80690.0                                Monday, 29 Feb 2016   \n",
       "56   81052.0                                Monday, 29 Feb 2016   \n",
       "111  80994.0                                Tuesday, 1 Mar 2016   \n",
       "726    311.0  Microsoft offering £100 discount on original B...   \n",
       "727    252.0  Economy to improve though no change in last 6 ...   \n",
       "\n",
       "                                              Headline  \\\n",
       "6    RAMALLAH, February 25, 2016 (WAFA) - Palestine...   \n",
       "56   RAMALLAH, February 29, 2016 (WAFA) - The Gover...   \n",
       "111  RAMALLAH, February 29, 2016 (WAFA) - The Gover...   \n",
       "726  Just ahead of the market rollout of Microsoft'...   \n",
       "727  &quot;In the coming six months, there seems to...   \n",
       "\n",
       "                                                Source      Topic  \\\n",
       "6                                                  NaN  palestine   \n",
       "56                                                 NaN  palestine   \n",
       "111                                                NaN  palestine   \n",
       "726  International Business Times via Yahoo UK & Ir...  microsoft   \n",
       "727  IANS India Private Limited/Yahoo India News vi...    economy   \n",
       "\n",
       "             PublishDate  SentimentTitle  SentimentHeadline  Facebook  \\\n",
       "6    2016-02-28 14:03:00        0.000000          -0.005906         0   \n",
       "56   2016-03-01 09:29:00        0.000000           0.048546         0   \n",
       "111  2016-03-01 00:15:00       -0.243068           0.048546         0   \n",
       "726  2015-11-08 05:25:00        0.000000          -0.132812         0   \n",
       "727  2015-11-08 05:40:00       -0.082022           0.205537         0   \n",
       "\n",
       "     GooglePlus  LinkedIn  \n",
       "6             0         0  \n",
       "56            0         0  \n",
       "111           0         0  \n",
       "726           0         0  \n",
       "727           0         0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasource.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the description and distribution of the LinkedIn popularity column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    87494.000000\n",
       "mean        17.700185\n",
       "std        159.381923\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          4.000000\n",
       "max      20341.000000\n",
       "Name: LinkedIn, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasource['LinkedIn'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource['Date_Name'] = pd.to_datetime(datasource[\"PublishDate\"]).dt.strftime(\"%Y%m%d\")\n",
    "datasource[['Title', 'Headline','Source','Topic','PublishDate','Date_Name']] = datasource[['Title', 'Headline','Source','Topic','PublishDate','Date_Name']].astype(str)\n",
    "datasource.columns = ['IDLInk', 'Title', 'Headline', 'Source', 'Topic', 'PublishDate', 'SentimentTitle', 'SentimentHeadline', 'Facebook', 'GooglePlus', 'LinkedIn','Date_Name']\n",
    "#datasource.columns\n",
    "news_df = spark.createDataFrame(datasource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IDLInk',\n",
       " 'Title',\n",
       " 'Headline',\n",
       " 'Source',\n",
       " 'Topic',\n",
       " 'PublishDate',\n",
       " 'SentimentTitle',\n",
       " 'SentimentHeadline',\n",
       " 'Facebook',\n",
       " 'GooglePlus',\n",
       " 'LinkedIn',\n",
       " 'Date_Name']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+--------------------+------+---------+-------------------+--------------+--------------------+--------+----------+--------+---------+\n",
      "| IDLink|              Title|            Headline|Source|    Topic|        PublishDate|SentimentTitle|   SentimentHeadline|Facebook|GooglePlus|LinkedIn|Date_Name|\n",
      "+-------+-------------------+--------------------+------+---------+-------------------+--------------+--------------------+--------+----------+--------+---------+\n",
      "|80690.0|monday, 29 feb 2016|ramallah, februar...|   nan|palestine|2016-02-28 14:03:00|           0.0|-0.00590569489076...|       0|         0|       0| 20160228|\n",
      "|81052.0|monday, 29 feb 2016|ramallah, februar...|   nan|palestine|2016-03-01 09:29:00|           0.0|  0.0485459183673469|       0|         0|       0| 20160301|\n",
      "+-------+-------------------+--------------------+------+---------+-------------------+--------------+--------------------+--------+----------+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converting to Lowercase\n",
    "\n",
    "news_lower_df = news_df.select('IDLink',fn.lower(fn.col('Title')).alias('Title'),fn.lower(fn.col('Headline')).alias('Headline'),'Source',\n",
    "                               'Topic','PublishDate', 'SentimentTitle', 'SentimentHeadline', 'Facebook', 'GooglePlus', 'LinkedIn','Date_Name')\n",
    "                               \n",
    "news_lower_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Text String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IDLink: double (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Headline: string (nullable = true)\n",
      " |-- Source: string (nullable = true)\n",
      " |-- Topic: string (nullable = true)\n",
      " |-- PublishDate: string (nullable = true)\n",
      " |-- SentimentTitle: double (nullable = true)\n",
      " |-- SentimentHeadline: double (nullable = true)\n",
      " |-- Facebook: long (nullable = true)\n",
      " |-- GooglePlus: long (nullable = true)\n",
      " |-- LinkedIn: long (nullable = true)\n",
      " |-- Date_Name: string (nullable = true)\n",
      " |-- Title_Cleaned: string (nullable = true)\n",
      "\n",
      "+-------+--------------------+--------------------+--------------------+---------+-------------------+--------------------+--------------------+--------+----------+--------+---------+--------------------+\n",
      "| IDLink|               Title|            Headline|              Source|    Topic|        PublishDate|      SentimentTitle|   SentimentHeadline|Facebook|GooglePlus|LinkedIn|Date_Name|       Title_Cleaned|\n",
      "+-------+--------------------+--------------------+--------------------+---------+-------------------+--------------------+--------------------+--------+----------+--------+---------+--------------------+\n",
      "|80690.0| monday, 29 feb 2016|ramallah, februar...|                 nan|palestine|2016-02-28 14:03:00|                 0.0|-0.00590569489076...|       0|         0|       0| 20160228|        monday  feb |\n",
      "|81052.0| monday, 29 feb 2016|ramallah, februar...|                 nan|palestine|2016-03-01 09:29:00|                 0.0|  0.0485459183673469|       0|         0|       0| 20160301|        monday  feb |\n",
      "|80994.0| tuesday, 1 mar 2016|ramallah, februar...|                 nan|palestine|2016-03-01 00:15:00|  -0.243067956032876|  0.0485459183673469|       0|         0|       0| 20160301|       tuesday  mar |\n",
      "|  311.0|microsoft offerin...|just ahead of the...|International Bus...|microsoft|2015-11-08 05:25:00|                 0.0|          -0.1328125|       0|         0|       0| 20151108|microsoft offerin...|\n",
      "|  252.0|economy to improv...|&quot;in the comi...|IANS India Privat...|  economy|2015-11-08 05:40:00| -0.0820215768106176| 0.20553735831523898|       0|         0|       0| 20151108|economy to improv...|\n",
      "|  227.0|economy to improv...|new delhi, nov 8 ...|IANS via Yahoo Ma...|  economy|2015-11-08 05:48:00| -0.0820215768106176| 0.18696844106993998|       0|         0|       0| 20151108|economy to improv...|\n",
      "|  307.0|microsoft scales ...|new york — micros...|       The Columbian|microsoft|2015-11-08 06:54:00|   0.106302508033845|-0.20733299759354604|       0|         0|       0| 20151108|microsoft scales ...|\n",
      "|  207.0|idc predicts the ...|kuching: the glob...|     The Borneo Post|  economy|2015-11-08 08:34:00|                 0.0|0.003189439769249...|       1|         0|       0| 20151108|idc predicts the ...|\n",
      "|  203.0|economic preview:...|seems the economy...|        Market Watch|  economy|2015-11-08 09:15:00|  -0.232543858423599|  0.0486135912065751|      29|         0|       0| 20151108|economic preview ...|\n",
      "|  299.0|microsoft’s onedr...|when microsoft an...|Digital Trends vi...|microsoft|2015-11-08 12:15:00|-0.16613855402461902|  -0.259051711303503|       6|         0|       1| 20151108|microsofts onedri...|\n",
      "+-------+--------------------+--------------------+--------------------+---------+-------------------+--------------------+--------------------+--------+----------+--------+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lower, regexp_replace, split\n",
    "\n",
    "def clean_text(c):\n",
    "  c = lower(c)\n",
    "  c = regexp_replace(c, \"^rt \", \"\")\n",
    "  c = regexp_replace(c, \"(https?\\://)\\S+\", \"\")\n",
    "  c = regexp_replace(c, \"[^a-zA-Z0-9\\\\s]\", \"\")\n",
    "  c = regexp_replace(c,\"[\\d]+\",\"\")  \n",
    "  #c = split(c, \"\\\\s+\") tokenization...\n",
    "  return c\n",
    "\n",
    "clean_news_df = news_lower_df.withColumn('Title_Cleaned',clean_text(col('Title')))\n",
    "\n",
    "clean_news_df.printSchema()\n",
    "clean_news_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IDLink: double (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Headline: string (nullable = true)\n",
      " |-- Source: string (nullable = true)\n",
      " |-- Topic: string (nullable = true)\n",
      " |-- PublishDate: string (nullable = true)\n",
      " |-- SentimentTitle: double (nullable = true)\n",
      " |-- SentimentHeadline: double (nullable = true)\n",
      " |-- Facebook: long (nullable = true)\n",
      " |-- GooglePlus: long (nullable = true)\n",
      " |-- LinkedIn: long (nullable = true)\n",
      " |-- Date_Name: string (nullable = true)\n",
      " |-- Title_Cleaned: string (nullable = true)\n",
      " |-- vector: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+-------+--------------------+--------------------+--------------------+---------+-------------------+--------------------+--------------------+--------+----------+--------+---------+--------------------+--------------------+\n",
      "| IDLink|               Title|            Headline|              Source|    Topic|        PublishDate|      SentimentTitle|   SentimentHeadline|Facebook|GooglePlus|LinkedIn|Date_Name|       Title_Cleaned|              vector|\n",
      "+-------+--------------------+--------------------+--------------------+---------+-------------------+--------------------+--------------------+--------+----------+--------+---------+--------------------+--------------------+\n",
      "|80690.0| monday, 29 feb 2016|ramallah, februar...|                 nan|palestine|2016-02-28 14:03:00|                 0.0|-0.00590569489076...|       0|         0|       0| 20160228|        monday  feb |     [monday, , feb]|\n",
      "|81052.0| monday, 29 feb 2016|ramallah, februar...|                 nan|palestine|2016-03-01 09:29:00|                 0.0|  0.0485459183673469|       0|         0|       0| 20160301|        monday  feb |     [monday, , feb]|\n",
      "|80994.0| tuesday, 1 mar 2016|ramallah, februar...|                 nan|palestine|2016-03-01 00:15:00|  -0.243067956032876|  0.0485459183673469|       0|         0|       0| 20160301|       tuesday  mar |    [tuesday, , mar]|\n",
      "|  311.0|microsoft offerin...|just ahead of the...|International Bus...|microsoft|2015-11-08 05:25:00|                 0.0|          -0.1328125|       0|         0|       0| 20151108|microsoft offerin...|[microsoft, offer...|\n",
      "|  252.0|economy to improv...|&quot;in the comi...|IANS India Privat...|  economy|2015-11-08 05:40:00| -0.0820215768106176| 0.20553735831523898|       0|         0|       0| 20151108|economy to improv...|[economy, to, imp...|\n",
      "|  227.0|economy to improv...|new delhi, nov 8 ...|IANS via Yahoo Ma...|  economy|2015-11-08 05:48:00| -0.0820215768106176| 0.18696844106993998|       0|         0|       0| 20151108|economy to improv...|[economy, to, imp...|\n",
      "|  307.0|microsoft scales ...|new york — micros...|       The Columbian|microsoft|2015-11-08 06:54:00|   0.106302508033845|-0.20733299759354604|       0|         0|       0| 20151108|microsoft scales ...|[microsoft, scale...|\n",
      "|  207.0|idc predicts the ...|kuching: the glob...|     The Borneo Post|  economy|2015-11-08 08:34:00|                 0.0|0.003189439769249...|       1|         0|       0| 20151108|idc predicts the ...|[idc, predicts, t...|\n",
      "|  203.0|economic preview:...|seems the economy...|        Market Watch|  economy|2015-11-08 09:15:00|  -0.232543858423599|  0.0486135912065751|      29|         0|       0| 20151108|economic preview ...|[economic, previe...|\n",
      "|  299.0|microsoft’s onedr...|when microsoft an...|Digital Trends vi...|microsoft|2015-11-08 12:15:00|-0.16613855402461902|  -0.259051711303503|       6|         0|       1| 20151108|microsofts onedri...|[microsofts, oned...|\n",
      "+-------+--------------------+--------------------+--------------------+---------+-------------------+--------------------+--------------------+--------+----------+--------+---------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"Title_Cleaned\", outputCol=\"vector\")\n",
    "title_vector_df = tokenizer.transform(clean_news_df)\n",
    "\n",
    "title_vector_df.printSchema()\n",
    "title_vector_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP WORDS REMOVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# Define a list of stop words or use default list\n",
    "remover = StopWordsRemover()\n",
    "stopwords = remover.getStopWords() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IDLink: double (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Headline: string (nullable = true)\n",
      " |-- Source: string (nullable = true)\n",
      " |-- Topic: string (nullable = true)\n",
      " |-- PublishDate: string (nullable = true)\n",
      " |-- SentimentTitle: double (nullable = true)\n",
      " |-- SentimentHeadline: double (nullable = true)\n",
      " |-- Facebook: long (nullable = true)\n",
      " |-- GooglePlus: long (nullable = true)\n",
      " |-- LinkedIn: long (nullable = true)\n",
      " |-- Date_Name: string (nullable = true)\n",
      " |-- Title_Cleaned: string (nullable = true)\n",
      " |-- vector: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- vector_no_stopw: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+-------+--------------------+--------------------+--------------------+---------+-------------------+--------------------+--------------------+--------+----------+--------+---------+--------------------+--------------------+--------------------+\n",
      "| IDLink|               Title|            Headline|              Source|    Topic|        PublishDate|      SentimentTitle|   SentimentHeadline|Facebook|GooglePlus|LinkedIn|Date_Name|       Title_Cleaned|              vector|     vector_no_stopw|\n",
      "+-------+--------------------+--------------------+--------------------+---------+-------------------+--------------------+--------------------+--------+----------+--------+---------+--------------------+--------------------+--------------------+\n",
      "|80690.0| monday, 29 feb 2016|ramallah, februar...|                 nan|palestine|2016-02-28 14:03:00|                 0.0|-0.00590569489076...|       0|         0|       0| 20160228|        monday  feb |     [monday, , feb]|     [monday, , feb]|\n",
      "|81052.0| monday, 29 feb 2016|ramallah, februar...|                 nan|palestine|2016-03-01 09:29:00|                 0.0|  0.0485459183673469|       0|         0|       0| 20160301|        monday  feb |     [monday, , feb]|     [monday, , feb]|\n",
      "|80994.0| tuesday, 1 mar 2016|ramallah, februar...|                 nan|palestine|2016-03-01 00:15:00|  -0.243067956032876|  0.0485459183673469|       0|         0|       0| 20160301|       tuesday  mar |    [tuesday, , mar]|    [tuesday, , mar]|\n",
      "|  311.0|microsoft offerin...|just ahead of the...|International Bus...|microsoft|2015-11-08 05:25:00|                 0.0|          -0.1328125|       0|         0|       0| 20151108|microsoft offerin...|[microsoft, offer...|[microsoft, offer...|\n",
      "|  252.0|economy to improv...|&quot;in the comi...|IANS India Privat...|  economy|2015-11-08 05:40:00| -0.0820215768106176| 0.20553735831523898|       0|         0|       0| 20151108|economy to improv...|[economy, to, imp...|[economy, improve...|\n",
      "|  227.0|economy to improv...|new delhi, nov 8 ...|IANS via Yahoo Ma...|  economy|2015-11-08 05:48:00| -0.0820215768106176| 0.18696844106993998|       0|         0|       0| 20151108|economy to improv...|[economy, to, imp...|[economy, improve...|\n",
      "|  307.0|microsoft scales ...|new york — micros...|       The Columbian|microsoft|2015-11-08 06:54:00|   0.106302508033845|-0.20733299759354604|       0|         0|       0| 20151108|microsoft scales ...|[microsoft, scale...|[microsoft, scale...|\n",
      "|  207.0|idc predicts the ...|kuching: the glob...|     The Borneo Post|  economy|2015-11-08 08:34:00|                 0.0|0.003189439769249...|       1|         0|       0| 20151108|idc predicts the ...|[idc, predicts, t...|[idc, predicts, e...|\n",
      "|  203.0|economic preview:...|seems the economy...|        Market Watch|  economy|2015-11-08 09:15:00|  -0.232543858423599|  0.0486135912065751|      29|         0|       0| 20151108|economic preview ...|[economic, previe...|[economic, previe...|\n",
      "|  299.0|microsoft’s onedr...|when microsoft an...|Digital Trends vi...|microsoft|2015-11-08 12:15:00|-0.16613855402461902|  -0.259051711303503|       6|         0|       1| 20151108|microsofts onedri...|[microsofts, oned...|[microsofts, oned...|\n",
      "|  294.0|‘economy to impro...|in the coming six...|           The Hindu|  economy|2015-11-08 12:54:00|   0.114819831692961|   0.256116463436781|       2|         0|       3| 20151108|economy to improv...|[economy, to, imp...|[economy, improve...|\n",
      "|  209.0|now comes fedspea...|the us economy ha...|Business Insider ...|  economy|2015-11-08 13:00:00|-0.00144453176100311|-0.16137430609197598|       1|         0|       0| 20151108|now comes fedspea...|[now, comes, feds...|[comes, fedspeak,...|\n",
      "|  292.0|get ready for a t...|the us economy ha...|    Business Insider|  economy|2015-11-08 13:07:00| -0.0559021431000154|   -0.37892704249222|      27|         2|      22| 20151108|get ready for a t...|[get, ready, for,...|[get, ready, ton,...|\n",
      "|  272.0|pak economy from ...|the latest annual...|   Business Recorder|  economy|2015-11-08 15:23:00|-0.16572815184059697|   0.177022048218733|       0|         0|       0| 20151108|pak economy from ...|[pak, economy, fr...|[pak, economy, fi...|\n",
      "|  328.0|microsoft to play...|bhaskar pramanik,...|           DNA India|microsoft|2015-11-08 16:47:00| -0.0183256172839505|              0.0625|      11|         1|       1| 20151108|microsoft to play...|[microsoft, to, p...|[microsoft, play,...|\n",
      "|  232.0|services pull sco...|the scottish priv...|          The Herald|  economy|2015-11-08 16:57:00| -0.0734930919740162|  -0.220573907796242|       0|         0|       0| 20151108|services pull sco...|[services, pull, ...|[services, pull, ...|\n",
      "|  587.0|microsoft risks i...| microsoft has ma...|            MIS Asia|microsoft|2015-11-08 18:54:00|-0.16572815184059697|   0.146389848356689|      89|         0|       0| 20151108|microsoft risks i...|[microsoft, risks...|[microsoft, risks...|\n",
      "|  201.0|dollar goes from ...|zimbabwe freed it...|           Bloomberg|  economy|2015-11-08 20:41:00| -0.0790569415042095|                 0.0|      61|         0|      32| 20151108|dollar goes from ...|[dollar, goes, fr...|[dollar, goes, sa...|\n",
      "|  476.0|microsoft readies...|microsoft is on t...|        SiliconANGLE|microsoft|2015-11-08 21:50:00|0.004359567901234...|   0.210429792569898|       4|         2|       4| 20151108|microsoft readies...|[microsoft, readi...|[microsoft, readi...|\n",
      "|  240.0|centre moves aggr...| indian governmen...|The Financial Exp...|  economy|2015-11-08 22:40:00| 0.22097086912079603| -0.0820041226391161|       0|         0|       0| 20151108|centre moves aggr...|[centre, moves, a...|[centre, moves, a...|\n",
      "+-------+--------------------+--------------------+--------------------+---------+-------------------+--------------------+--------------------+--------+----------+--------+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specify input/output columns\n",
    "remover.setInputCol(\"vector\")\n",
    "remover.setOutputCol(\"vector_no_stopw\")\n",
    "\n",
    "# Transform existing dataframe with the StopWordsRemover\n",
    "Titlevector_no_stopw_df = remover.transform(title_vector_df)\n",
    "\n",
    "# Display\n",
    "Titlevector_no_stopw_df.printSchema()\n",
    "Titlevector_no_stopw_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIGRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IDLink: double (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Headline: string (nullable = true)\n",
      " |-- Source: string (nullable = true)\n",
      " |-- Topic: string (nullable = true)\n",
      " |-- PublishDate: string (nullable = true)\n",
      " |-- SentimentTitle: double (nullable = true)\n",
      " |-- SentimentHeadline: double (nullable = true)\n",
      " |-- Facebook: long (nullable = true)\n",
      " |-- GooglePlus: long (nullable = true)\n",
      " |-- LinkedIn: long (nullable = true)\n",
      " |-- Date_Name: string (nullable = true)\n",
      " |-- Title_Cleaned: string (nullable = true)\n",
      " |-- vector: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- vector_no_stopw: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- bigrams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n",
      "+-------+--------------------+--------------------+--------------------+---------+-------------------+--------------------+--------------------+--------+----------+--------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "| IDLink|               Title|            Headline|              Source|    Topic|        PublishDate|      SentimentTitle|   SentimentHeadline|Facebook|GooglePlus|LinkedIn|Date_Name|       Title_Cleaned|              vector|     vector_no_stopw|             bigrams|\n",
      "+-------+--------------------+--------------------+--------------------+---------+-------------------+--------------------+--------------------+--------+----------+--------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|80690.0| monday, 29 feb 2016|ramallah, februar...|                 nan|palestine|2016-02-28 14:03:00|                 0.0|-0.00590569489076...|       0|         0|       0| 20160228|        monday  feb |     [monday, , feb]|     [monday, , feb]|     [monday ,  feb]|\n",
      "|81052.0| monday, 29 feb 2016|ramallah, februar...|                 nan|palestine|2016-03-01 09:29:00|                 0.0|  0.0485459183673469|       0|         0|       0| 20160301|        monday  feb |     [monday, , feb]|     [monday, , feb]|     [monday ,  feb]|\n",
      "|80994.0| tuesday, 1 mar 2016|ramallah, februar...|                 nan|palestine|2016-03-01 00:15:00|  -0.243067956032876|  0.0485459183673469|       0|         0|       0| 20160301|       tuesday  mar |    [tuesday, , mar]|    [tuesday, , mar]|    [tuesday ,  mar]|\n",
      "|  311.0|microsoft offerin...|just ahead of the...|International Bus...|microsoft|2015-11-08 05:25:00|                 0.0|          -0.1328125|       0|         0|       0| 20151108|microsoft offerin...|[microsoft, offer...|[microsoft, offer...|[microsoft offeri...|\n",
      "|  252.0|economy to improv...|&quot;in the comi...|IANS India Privat...|  economy|2015-11-08 05:40:00| -0.0820215768106176| 0.20553735831523898|       0|         0|       0| 20151108|economy to improv...|[economy, to, imp...|[economy, improve...|[economy improve,...|\n",
      "|  227.0|economy to improv...|new delhi, nov 8 ...|IANS via Yahoo Ma...|  economy|2015-11-08 05:48:00| -0.0820215768106176| 0.18696844106993998|       0|         0|       0| 20151108|economy to improv...|[economy, to, imp...|[economy, improve...|[economy improve,...|\n",
      "|  307.0|microsoft scales ...|new york — micros...|       The Columbian|microsoft|2015-11-08 06:54:00|   0.106302508033845|-0.20733299759354604|       0|         0|       0| 20151108|microsoft scales ...|[microsoft, scale...|[microsoft, scale...|[microsoft scales...|\n",
      "|  207.0|idc predicts the ...|kuching: the glob...|     The Borneo Post|  economy|2015-11-08 08:34:00|                 0.0|0.003189439769249...|       1|         0|       0| 20151108|idc predicts the ...|[idc, predicts, t...|[idc, predicts, e...|[idc predicts, pr...|\n",
      "|  203.0|economic preview:...|seems the economy...|        Market Watch|  economy|2015-11-08 09:15:00|  -0.232543858423599|  0.0486135912065751|      29|         0|       0| 20151108|economic preview ...|[economic, previe...|[economic, previe...|[economic preview...|\n",
      "|  299.0|microsoft’s onedr...|when microsoft an...|Digital Trends vi...|microsoft|2015-11-08 12:15:00|-0.16613855402461902|  -0.259051711303503|       6|         0|       1| 20151108|microsofts onedri...|[microsofts, oned...|[microsofts, oned...|[microsofts onedr...|\n",
      "|  294.0|‘economy to impro...|in the coming six...|           The Hindu|  economy|2015-11-08 12:54:00|   0.114819831692961|   0.256116463436781|       2|         0|       3| 20151108|economy to improv...|[economy, to, imp...|[economy, improve...|[economy improve,...|\n",
      "|  209.0|now comes fedspea...|the us economy ha...|Business Insider ...|  economy|2015-11-08 13:00:00|-0.00144453176100311|-0.16137430609197598|       1|         0|       0| 20151108|now comes fedspea...|[now, comes, feds...|[comes, fedspeak,...|[comes fedspeak, ...|\n",
      "|  292.0|get ready for a t...|the us economy ha...|    Business Insider|  economy|2015-11-08 13:07:00| -0.0559021431000154|   -0.37892704249222|      27|         2|      22| 20151108|get ready for a t...|[get, ready, for,...|[get, ready, ton,...|[get ready, ready...|\n",
      "|  272.0|pak economy from ...|the latest annual...|   Business Recorder|  economy|2015-11-08 15:23:00|-0.16572815184059697|   0.177022048218733|       0|         0|       0| 20151108|pak economy from ...|[pak, economy, fr...|[pak, economy, fi...|[pak economy, eco...|\n",
      "|  328.0|microsoft to play...|bhaskar pramanik,...|           DNA India|microsoft|2015-11-08 16:47:00| -0.0183256172839505|              0.0625|      11|         1|       1| 20151108|microsoft to play...|[microsoft, to, p...|[microsoft, play,...|[microsoft play, ...|\n",
      "|  232.0|services pull sco...|the scottish priv...|          The Herald|  economy|2015-11-08 16:57:00| -0.0734930919740162|  -0.220573907796242|       0|         0|       0| 20151108|services pull sco...|[services, pull, ...|[services, pull, ...|[services pull, p...|\n",
      "|  587.0|microsoft risks i...| microsoft has ma...|            MIS Asia|microsoft|2015-11-08 18:54:00|-0.16572815184059697|   0.146389848356689|      89|         0|       0| 20151108|microsoft risks i...|[microsoft, risks...|[microsoft, risks...|[microsoft risks,...|\n",
      "|  201.0|dollar goes from ...|zimbabwe freed it...|           Bloomberg|  economy|2015-11-08 20:41:00| -0.0790569415042095|                 0.0|      61|         0|      32| 20151108|dollar goes from ...|[dollar, goes, fr...|[dollar, goes, sa...|[dollar goes, goe...|\n",
      "|  476.0|microsoft readies...|microsoft is on t...|        SiliconANGLE|microsoft|2015-11-08 21:50:00|0.004359567901234...|   0.210429792569898|       4|         2|       4| 20151108|microsoft readies...|[microsoft, readi...|[microsoft, readi...|[microsoft readie...|\n",
      "|  240.0|centre moves aggr...| indian governmen...|The Financial Exp...|  economy|2015-11-08 22:40:00| 0.22097086912079603| -0.0820041226391161|       0|         0|       0| 20151108|centre moves aggr...|[centre, moves, a...|[centre, moves, a...|[centre moves, mo...|\n",
      "+-------+--------------------+--------------------+--------------------+---------+-------------------+--------------------+--------------------+--------+----------+--------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "# Define NGram transformer\n",
    "ngram = NGram(n=2, inputCol=\"vector_no_stopw\", outputCol=\"bigrams\")\n",
    "\n",
    "# Create bigram_df as a transform of unigram_df using NGram tranformer\n",
    "production_df = ngram.transform(Titlevector_no_stopw_df)\n",
    "\n",
    "# Display\n",
    "production_df.printSchema()\n",
    "production_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IDLink: double (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Headline: string (nullable = true)\n",
      " |-- Source: string (nullable = true)\n",
      " |-- Topic: string (nullable = true)\n",
      " |-- PublishDate: string (nullable = true)\n",
      " |-- SentimentTitle: double (nullable = true)\n",
      " |-- SentimentHeadline: double (nullable = true)\n",
      " |-- Facebook: long (nullable = true)\n",
      " |-- GooglePlus: long (nullable = true)\n",
      " |-- LinkedIn: long (nullable = true)\n",
      " |-- Date_Name: string (nullable = true)\n",
      " |-- Title_Cleaned: string (nullable = true)\n",
      " |-- vector: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- vector_no_stopw: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- bigrams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n",
      "+-------+--------------------+--------------------+--------------------+---------+-------------------+--------------------+--------------------+--------+----------+--------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "| IDLink|               Title|            Headline|              Source|    Topic|        PublishDate|      SentimentTitle|   SentimentHeadline|Facebook|GooglePlus|LinkedIn|Date_Name|       Title_Cleaned|              vector|     vector_no_stopw|             bigrams|\n",
      "+-------+--------------------+--------------------+--------------------+---------+-------------------+--------------------+--------------------+--------+----------+--------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|80690.0| monday, 29 feb 2016|ramallah, februar...|                 nan|palestine|2016-02-28 14:03:00|                 0.0|-0.00590569489076...|       0|         0|       0| 20160228|        monday  feb |     [monday, , feb]|     [monday, , feb]|     [monday ,  feb]|\n",
      "|81052.0| monday, 29 feb 2016|ramallah, februar...|                 nan|palestine|2016-03-01 09:29:00|                 0.0|  0.0485459183673469|       0|         0|       0| 20160301|        monday  feb |     [monday, , feb]|     [monday, , feb]|     [monday ,  feb]|\n",
      "|80994.0| tuesday, 1 mar 2016|ramallah, februar...|                 nan|palestine|2016-03-01 00:15:00|  -0.243067956032876|  0.0485459183673469|       0|         0|       0| 20160301|       tuesday  mar |    [tuesday, , mar]|    [tuesday, , mar]|    [tuesday ,  mar]|\n",
      "|  311.0|microsoft offerin...|just ahead of the...|International Bus...|microsoft|2015-11-08 05:25:00|                 0.0|          -0.1328125|       0|         0|       0| 20151108|microsoft offerin...|[microsoft, offer...|[microsoft, offer...|[microsoft offeri...|\n",
      "|  252.0|economy to improv...|&quot;in the comi...|IANS India Privat...|  economy|2015-11-08 05:40:00| -0.0820215768106176| 0.20553735831523898|       0|         0|       0| 20151108|economy to improv...|[economy, to, imp...|[economy, improve...|[economy improve,...|\n",
      "|  227.0|economy to improv...|new delhi, nov 8 ...|IANS via Yahoo Ma...|  economy|2015-11-08 05:48:00| -0.0820215768106176| 0.18696844106993998|       0|         0|       0| 20151108|economy to improv...|[economy, to, imp...|[economy, improve...|[economy improve,...|\n",
      "|  307.0|microsoft scales ...|new york — micros...|       The Columbian|microsoft|2015-11-08 06:54:00|   0.106302508033845|-0.20733299759354604|       0|         0|       0| 20151108|microsoft scales ...|[microsoft, scale...|[microsoft, scale...|[microsoft scales...|\n",
      "|  207.0|idc predicts the ...|kuching: the glob...|     The Borneo Post|  economy|2015-11-08 08:34:00|                 0.0|0.003189439769249...|       1|         0|       0| 20151108|idc predicts the ...|[idc, predicts, t...|[idc, predicts, e...|[idc predicts, pr...|\n",
      "|  203.0|economic preview:...|seems the economy...|        Market Watch|  economy|2015-11-08 09:15:00|  -0.232543858423599|  0.0486135912065751|      29|         0|       0| 20151108|economic preview ...|[economic, previe...|[economic, previe...|[economic preview...|\n",
      "|  299.0|microsoft’s onedr...|when microsoft an...|Digital Trends vi...|microsoft|2015-11-08 12:15:00|-0.16613855402461902|  -0.259051711303503|       6|         0|       1| 20151108|microsofts onedri...|[microsofts, oned...|[microsofts, oned...|[microsofts onedr...|\n",
      "|  294.0|‘economy to impro...|in the coming six...|           The Hindu|  economy|2015-11-08 12:54:00|   0.114819831692961|   0.256116463436781|       2|         0|       3| 20151108|economy to improv...|[economy, to, imp...|[economy, improve...|[economy improve,...|\n",
      "|  209.0|now comes fedspea...|the us economy ha...|Business Insider ...|  economy|2015-11-08 13:00:00|-0.00144453176100311|-0.16137430609197598|       1|         0|       0| 20151108|now comes fedspea...|[now, comes, feds...|[comes, fedspeak,...|[comes fedspeak, ...|\n",
      "|  292.0|get ready for a t...|the us economy ha...|    Business Insider|  economy|2015-11-08 13:07:00| -0.0559021431000154|   -0.37892704249222|      27|         2|      22| 20151108|get ready for a t...|[get, ready, for,...|[get, ready, ton,...|[get ready, ready...|\n",
      "|  272.0|pak economy from ...|the latest annual...|   Business Recorder|  economy|2015-11-08 15:23:00|-0.16572815184059697|   0.177022048218733|       0|         0|       0| 20151108|pak economy from ...|[pak, economy, fr...|[pak, economy, fi...|[pak economy, eco...|\n",
      "|  328.0|microsoft to play...|bhaskar pramanik,...|           DNA India|microsoft|2015-11-08 16:47:00| -0.0183256172839505|              0.0625|      11|         1|       1| 20151108|microsoft to play...|[microsoft, to, p...|[microsoft, play,...|[microsoft play, ...|\n",
      "|  232.0|services pull sco...|the scottish priv...|          The Herald|  economy|2015-11-08 16:57:00| -0.0734930919740162|  -0.220573907796242|       0|         0|       0| 20151108|services pull sco...|[services, pull, ...|[services, pull, ...|[services pull, p...|\n",
      "|  587.0|microsoft risks i...| microsoft has ma...|            MIS Asia|microsoft|2015-11-08 18:54:00|-0.16572815184059697|   0.146389848356689|      89|         0|       0| 20151108|microsoft risks i...|[microsoft, risks...|[microsoft, risks...|[microsoft risks,...|\n",
      "|  201.0|dollar goes from ...|zimbabwe freed it...|           Bloomberg|  economy|2015-11-08 20:41:00| -0.0790569415042095|                 0.0|      61|         0|      32| 20151108|dollar goes from ...|[dollar, goes, fr...|[dollar, goes, sa...|[dollar goes, goe...|\n",
      "|  476.0|microsoft readies...|microsoft is on t...|        SiliconANGLE|microsoft|2015-11-08 21:50:00|0.004359567901234...|   0.210429792569898|       4|         2|       4| 20151108|microsoft readies...|[microsoft, readi...|[microsoft, readi...|[microsoft readie...|\n",
      "|  240.0|centre moves aggr...| indian governmen...|The Financial Exp...|  economy|2015-11-08 22:40:00| 0.22097086912079603| -0.0820041226391161|       0|         0|       0| 20151108|centre moves aggr...|[centre, moves, a...|[centre, moves, a...|[centre moves, mo...|\n",
      "+-------+--------------------+--------------------+--------------------+---------+-------------------+--------------------+--------------------+--------+----------+--------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, size\n",
    "\n",
    "production_df = production_df.where(size(col(\"bigrams\")) >= 2)\n",
    "\n",
    "# Display\n",
    "production_df.printSchema()\n",
    "production_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing and One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALTERING THE TIME STAMP\n",
    "\n",
    "production_df2 = production_df.withColumn('Date_Name',production_df[\"Date_Name\"].cast(\"float\") )\n",
    "\n",
    "\n",
    "#ONE HOT ENCODING\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Topic\", outputCol=\"Topic_numeric\").fit(production_df2)\n",
    "indexed_df = indexer.transform(production_df2)\n",
    "\n",
    "#type(indexed_df[\"Topic_numeric\"])\n",
    "\n",
    "\n",
    "encoder = OneHotEncoderEstimator(\n",
    "    inputCols=[\"Topic_numeric\"],  \n",
    "    outputCols=[\"Topic_vector\"]\n",
    ")\n",
    "\n",
    "\n",
    "model = encoder.fit(indexed_df)\n",
    "\n",
    "production_df2 = model.transform(indexed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word 2 Vec of Title Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title_Word2Vec = feature.Word2Vec(vectorSize=100, minCount=10, inputCol=\"vector_no_stopw\", outputCol=\"Title_Vect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Flow \n",
    "\n",
    "<br>\n",
    "<center> <img src=\"Data Flow.PNG\" width=\"60%\" align=\"center\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION ON BIGRAMS CREATED FROM TITLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df, validation_df, testing_df = production_df2.randomSplit([0.6, 0.3, 0.1], seed=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_estimator = CountVectorizer(minTF=1., minDF=5).setInputCol('bigrams').setOutputCol('tf_Title')\n",
    "count_vectorizer_idf = IDF().setInputCol(\"tf_Title\").setOutputCol(\"tfidf_Title\")\n",
    "Vector_Assembler = feature.VectorAssembler(inputCols=['tfidf_Title']\n",
    "                                           , outputCol='features')\n",
    "tfidf_stages_pipeline = Pipeline(stages=[count_vectorizer_estimator,count_vectorizer_idf,Vector_Assembler])\n",
    "\n",
    "pipe2_model = Pipeline(stages=[\n",
    "  tfidf_stages_pipeline,regression.LinearRegression(featuresCol='features', labelCol='LinkedIn')\n",
    "]).fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|LinkedIn|        prediction|\n",
      "+--------+------------------+\n",
      "|       2|-18.21342537053431|\n",
      "|      10| 9.973375817108572|\n",
      "|       0| 45.59153250468265|\n",
      "|       0| 4.895033277743437|\n",
      "|       5|13.076119445521728|\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_df = pipe2_model.transform(validation_df).select('LinkedIn','prediction')\n",
    "prediction_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             rmse|\n",
      "+-----------------+\n",
      "|184.7765144947246|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rmse_df = prediction_df.select((fn.col('prediction')- fn.col('LinkedIn')).alias('e')).\\\n",
    "    select(fn.sqrt(fn.avg(fn.col('e')**2)).alias(\"rmse\"))\n",
    "\n",
    "rmse_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "- Following Models were executed using Linear Regression\n",
    "\n",
    "<br>\n",
    "<center> <img src=\"LR.PNG\" width=\"60%\" align=\"center\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model 1-TitleTFIDF + Topic Vector(One hot) \n",
    "- Model 2 - TitleTFIDF + Topic Vector(Dummy)\n",
    "- Model 3- TitleTFIDF + Topic Vector(One Hot) + Publish Date \n",
    "- Model 4 –Regularization (TitleTFIDF + Topic Vector(One Hot) + Publish Date)\n",
    "- Model 5 – Regularization(TitleTFIDF + Topic Vector(One Hot)+ Publish Date + Sentiment Title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1-TitleTFIDF + Topic Vector(One hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe_model = Pipeline(stages=[\n",
    "  feature.VectorAssembler(inputCols=['Title_tfidf', 'Topic_vector'], outputCol='features'),\n",
    "  #feature.StandardScaler(withStd=False, withMean=True, inputCol='features', outputCol='features_scaled'),\n",
    "  regression.LinearRegression(featuresCol='features', labelCol='LinkedIn')  \n",
    "]).fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|LinkedIn|        prediction|\n",
      "+--------+------------------+\n",
      "|       2|-32.33004175140637|\n",
      "|      10| 6.732539069713411|\n",
      "|       0|24.849754068440873|\n",
      "|       0|17.654295709557385|\n",
      "|       5|-17.90697049743175|\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_df = pipe_model.transform(validation_df).select('LinkedIn','prediction')\n",
    "prediction_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_df = prediction_df.select((fn.col('prediction')- fn.col('LinkedIn')).alias('e')).\\\n",
    "    select(fn.sqrt(fn.avg(fn.col('e')**2)).alias(\"rmse\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              rmse|\n",
      "+------------------+\n",
      "|130.86717301955426|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rmse_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 - TitleTFIDF + Topic Vector(Dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+--------------------+--------+\n",
      "|Topic_Obama|Topic_Palestine|Topic_Economy|         Title_tfidf|LinkedIn|\n",
      "+-----------+---------------+-------------+--------------------+--------+\n",
      "|          0|              1|            0|(28249,[814,1294]...|       0|\n",
      "|          0|              1|            0|(28249,[814,1294]...|       0|\n",
      "|          0|              1|            0|(28249,[898,2528]...|       0|\n",
      "|          0|              0|            0|(28249,[2,20,189,...|       0|\n",
      "|          0|              0|            1|(28249,[1,116,622...|       0|\n",
      "|          0|              0|            1|(28249,[1,116,622...|       0|\n",
      "|          0|              0|            0|(28249,[2,71,98,4...|       0|\n",
      "|          0|              0|            1|(28249,[1,1181,49...|       0|\n",
      "|          0|              0|            1|(28249,[1,7,342,1...|       0|\n",
      "|          0|              0|            0|(28249,[2,28,81,7...|       1|\n",
      "|          0|              0|            1|(28249,[1,622,492...|       3|\n",
      "|          0|              0|            1|(28249,[55,57,93,...|       0|\n",
      "|          0|              0|            1|(28249,[30,445,57...|      22|\n",
      "|          0|              0|            1|(28249,[1,34,903,...|       0|\n",
      "|          0|              0|            0|(28249,[2,32,57,1...|       1|\n",
      "|          0|              0|            1|(28249,[1,12,249,...|       0|\n",
      "|          0|              0|            0|(28249,[2,9,58,17...|       0|\n",
      "|          0|              0|            1|(28249,[1,315,494...|      32|\n",
      "|          0|              0|            0|(28249,[2,9,58,21...|       4|\n",
      "|          0|              0|            1|(28249,[1,162,473...|       0|\n",
      "+-----------+---------------+-------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_df = news_Title_Proc.select(fn.when(fn.col('Topic')=='obama',1).otherwise(0).alias('Topic_Obama'),\n",
    "                            fn.when(fn.col('Topic')=='palestine',1).otherwise(0).alias('Topic_Palestine'),\n",
    "                            fn.when(fn.col('Topic')=='economy',1).otherwise(0).alias('Topic_Economy'),\n",
    "                                  'Title_tfidf', 'LinkedIn'\n",
    "\n",
    "                          )\n",
    "dummy_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression with Title tfidf and topic with dummy variables\n",
    "pipe_model = Pipeline(stages=[\n",
    "  feature.VectorAssembler(inputCols=['Topic_Obama','Topic_Palestine', 'Topic_Economy', 'Title_tfidf'], outputCol='features'),\n",
    "  #feature.StandardScaler(withStd=False, withMean=True, inputCol='features', outputCol='features_scaled'),\n",
    "  regression.LinearRegression(featuresCol='features', labelCol='LinkedIn')  \n",
    "]).fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|LinkedIn|         prediction|\n",
      "+--------+-------------------+\n",
      "|       0|  75.09928578162172|\n",
      "|       0| 119.19894872441887|\n",
      "|       7| 119.19894872441887|\n",
      "|       0|-40.689224124744435|\n",
      "|      39| 31.757177713458137|\n",
      "+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_df = pipe_model.transform(validation_df).select('LinkedIn','prediction')\n",
    "prediction_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_df = prediction_df.select((fn.col('prediction')- fn.col('LinkedIn')).alias('e')).\\\n",
    "    select(fn.sqrt(fn.avg(fn.col('e')**2)).alias(\"rmse\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             rmse|\n",
      "+-----------------+\n",
      "|142.7637066211151|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rmse_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3- TitleTFIDF + Topic Vector(One Hot) + Publish Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression with Title tfidf, Topic_vector and Publishdate \n",
    "#Linear Regression\n",
    "pipe_model = Pipeline(stages=[\n",
    "  feature.VectorAssembler(inputCols=['Title_tfidf', 'Topic_vector', 'Date_Name'], outputCol='features'),\n",
    "  regression.LinearRegression(featuresCol='features', labelCol='LinkedIn')  \n",
    "]).fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|LinkedIn|         prediction|\n",
      "+--------+-------------------+\n",
      "|       2| -36.47149463005371|\n",
      "|      10| 0.8176401569544396|\n",
      "|       0|  20.37265079719691|\n",
      "|       0| 13.749502936228964|\n",
      "|       5|-23.285376369121877|\n",
      "+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_df = pipe_model.transform(validation_df).select('LinkedIn','prediction')\n",
    "prediction_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_df = prediction_df.select((fn.col('prediction')- fn.col('LinkedIn')).alias('e')).\\\n",
    "    select(fn.sqrt(fn.avg(fn.col('e')**2)).alias(\"rmse\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             rmse|\n",
      "+-----------------+\n",
      "|130.8979205266552|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rmse_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4 –Regularization (TitleTFIDF + Topic Vector(One Hot) + Publish Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression with Title tfidf, Topic_vector and Publishdate with regularization 1\n",
    "#Linear Regression\n",
    "pipe_model = Pipeline(stages=[\n",
    "  feature.VectorAssembler(inputCols=['Title_tfidf','Topic_vector' ,'Date_Name'], outputCol='features'),\n",
    "  #feature.StandardScaler(withStd=False, withMean=True, inputCol='features', outputCol='features_scaled'),\n",
    "  regression.LinearRegression(featuresCol='features', labelCol='LinkedIn')  \n",
    "    .setMaxIter(10)\n",
    "  .setRegParam(0.3)\n",
    "  .setElasticNetParam(0.8)\n",
    "]).fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|LinkedIn|         prediction|\n",
      "+--------+-------------------+\n",
      "|       2| -7.294303640389444|\n",
      "|      10|-2.4146784262975416|\n",
      "|       0|   4.09783572267861|\n",
      "|       0|  11.90262137483478|\n",
      "|       5| 1.0415837966957042|\n",
      "+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_df = pipe_model.transform(validation_df).select('LinkedIn','prediction')\n",
    "prediction_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_df = prediction_df.select((fn.col('prediction')- fn.col('LinkedIn')).alias('e')).\\\n",
    "    select(fn.sqrt(fn.avg(fn.col('e')**2)).alias(\"rmse\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              rmse|\n",
      "+------------------+\n",
      "|110.25568731465081|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rmse_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 5 – Regularization(TitleTFIDF + Topic Vector(One Hot)+ Publish Date + Sentiment Title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression with Title tfidf, Topic_vector and Publishdate with regularization 2\n",
    "#Linear Regression\n",
    "pipe_model = Pipeline(stages=[\n",
    "  feature.VectorAssembler(inputCols=['Title_tfidf','Topic_vector' ,'Date_Name', 'SentimentTitle'], outputCol='features'),\n",
    "  #feature.StandardScaler(withStd=False, withMean=True, inputCol='features', outputCol='features_scaled'),\n",
    "  regression.LinearRegression(featuresCol='features', labelCol='LinkedIn')  \n",
    "    .setMaxIter(10)\n",
    "  .setRegParam(0.3)\n",
    "  .setElasticNetParam(0.2)\n",
    "]).fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|LinkedIn|         prediction|\n",
      "+--------+-------------------+\n",
      "|       2|-24.727705272176536|\n",
      "|      10|-0.6823508163779479|\n",
      "|       0|  10.11249792613853|\n",
      "|       0| 12.537786014585436|\n",
      "|       5|-17.003457270870967|\n",
      "+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_df = pipe_model.transform(validation_df).select('LinkedIn','prediction')\n",
    "prediction_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_df = prediction_df.select((fn.col('prediction')- fn.col('LinkedIn')).alias('e')).\\\n",
    "    select(fn.sqrt(fn.avg(fn.col('e')**2)).alias(\"rmse\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              rmse|\n",
      "+------------------+\n",
      "|119.31515253397016|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rmse_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression with Title tfidf, Topic_vector, Publishdate and Sentiment Title without regularization\n",
    "#Linear Regression\n",
    "pipe_model = Pipeline(stages=[\n",
    "  feature.VectorAssembler(inputCols=['Title_tfidf','Topic_vector' ,'Date_Name','SentimentTitle'], outputCol='features'),\n",
    "  regression.LinearRegression(featuresCol='features', labelCol='LinkedIn')  \n",
    "]).fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|LinkedIn|         prediction|\n",
      "+--------+-------------------+\n",
      "|       2|-36.755872137748156|\n",
      "|      10|0.47840043248834263|\n",
      "|       0| 20.374118666653885|\n",
      "|       0| 13.418391002156568|\n",
      "|       5|-23.497855093584803|\n",
      "+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_df = pipe_model.transform(validation_df).select('LinkedIn','prediction')\n",
    "prediction_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_df = prediction_df.select((fn.col('prediction')- fn.col('LinkedIn')).alias('e')).\\\n",
    "    select(fn.sqrt(fn.avg(fn.col('e')**2)).alias(\"rmse\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              rmse|\n",
      "+------------------+\n",
      "|130.89102769356703|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rmse_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Linear Regression with Title tfidf, topic_vector, Publishdate and Sentiment title with regularization \n",
    "\n",
    "pipe_model = Pipeline(stages=[\n",
    "  feature.VectorAssembler(inputCols=['Title_tfidf','Topic_vector' ,'Date_Name', 'SentimentTitle'], outputCol='features'),\n",
    "  #feature.StandardScaler(withStd=False, withMean=True, inputCol='features', outputCol='features_scaled'),\n",
    "  regression.LinearRegression(featuresCol='features', labelCol='LinkedIn')  \n",
    "    .setMaxIter(20)\n",
    "  .setRegParam(2.0)\n",
    "  .setElasticNetParam(1.0)\n",
    "]).fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|LinkedIn|       prediction|\n",
      "+--------+-----------------+\n",
      "|       2|9.554063721972184|\n",
      "|      10|10.66254629135987|\n",
      "|       0|10.66254629135987|\n",
      "|       0|10.66254629135987|\n",
      "|       5|10.66254629135987|\n",
      "+--------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_df = pipe_model.transform(validation_df).select('LinkedIn','prediction')\n",
    "prediction_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_df = prediction_df.select((fn.col('prediction')- fn.col('LinkedIn')).alias('e')).\\\n",
    "    select(fn.sqrt(fn.avg(fn.col('e')**2)).alias(\"rmse\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             rmse|\n",
      "+-----------------+\n",
      "|96.37281336466918|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rmse_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<center> <img src=\"Coefficient.PNG\" width=\"50%\" align=\"center\"> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Coefficients for the model\n",
    "coeff=pipe_model.stages[1].coefficients.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.680860421441198"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff2=coeff[:-3]\n",
    "coeff3=coeff[-3:]\n",
    "np.array(coeff3)\n",
    "import numpy as np\n",
    "mean_coeffs = np.mean(coeff2)\n",
    "mean_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13.68086042, -15.24393408,  -0.99969282,   4.47218909])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_coeff=np.append(mean_coeffs , coeff3)\n",
    "final_coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation on Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.tuning as tune\n",
    "grid = tune.ParamGridBuilder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = regression.LinearRegression(labelCol = 'LinkedIn', featuresCol = 'features', maxIter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = grid.addGrid(reg.elasticNetParam, [0.6, 0.8, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = grid.addGrid(reg.regParam, np.arange(1,3,.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 1.5, 2. , 2.5])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(1,3,.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = grid.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(labelCol=reg.getLabelCol(), predictionCol=reg.getPredictionCol())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "va= feature.VectorAssembler(inputCols=['Title_tfidf','Topic_vector' ,'Date_Name', 'SentimentTitle'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossPipe = Pipeline(stages=[va,reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = tune.CrossValidator(estimator = crossPipe, estimatorParamMaps = grid, evaluator= evaluator, numFolds = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossValidatorVerbose(CrossValidator):\n",
    "      def _fit(self, dataset):\n",
    "        est = self.getOrDefault(self.estimator)\n",
    "        epm = self.getOrDefault(self.estimatorParamMaps)\n",
    "        numModels = len(epm)\n",
    "\n",
    "        eva = self.getOrDefault(self.evaluator)\n",
    "        metricName = eva.getMetricName()\n",
    "\n",
    "        nFolds = self.getOrDefault(self.numFolds)\n",
    "        seed = self.getOrDefault(self.seed)\n",
    "        h = 1.0 / nFolds\n",
    "\n",
    "        randCol = self.uid + \"_rand\"\n",
    "        df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
    "        metrics = [0.0] * numModels\n",
    "\n",
    "        for i in range(nFolds):\n",
    "            foldNum = i + 1\n",
    "            print(\"Comparing models on fold %d\" % foldNum)\n",
    "            validateLB = i * h\n",
    "            validateUB = (i + 1) * h\n",
    "            condition = (df[randCol] >= validateLB) & (df[randCol] < validateUB)\n",
    "            validation = df.filter(condition)\n",
    "            train = df.filter(~condition)\n",
    "\n",
    "            for j in range(numModels):\n",
    "                paramMap = epm[j]\n",
    "                model = est.fit(train, paramMap)\n",
    "                # TODO: duplicate evaluator to take extra params from input\n",
    "                metric = eva.evaluate(model.transform(validation, paramMap))\n",
    "                metrics[j] += metric\n",
    "\n",
    "                avgSoFar = metrics[j] / foldNum\n",
    "                print(\"params: %s\\t%s: %f\\tavg: %f\" % (\n",
    "                    {param.name: val for (param, val) in paramMap.items()},\n",
    "                    metricName, metric, avgSoFar))\n",
    "                list1.append([{param.name: val for (param, val) in paramMap.items()},metric,avgSoFar])\n",
    "              # paramsList.append([[{param.name: val for (param, val) in paramMap.items()},metricName,metric,avgSoFar]])\n",
    "\n",
    "        if eva.isLargerBetter():\n",
    "            bestIndex = np.argmax(metrics)\n",
    "        else:\n",
    "            bestIndex = np.argmin(metrics)\n",
    "\n",
    "        bestParams = epm[bestIndex]\n",
    "        bestModel = est.fit(dataset, bestParams)\n",
    "        avgMetrics = [m / nFolds for m in metrics]\n",
    "        bestAvg = avgMetrics[bestIndex]\n",
    "        print(\"Best model:\\nparams: %s\\t%s: %f\" % (\n",
    "            {param.name: val for (param, val) in bestParams.items()},\n",
    "            metricName, bestAvg))\n",
    "\n",
    "        return self._copyValues(CrossValidatorModel(bestModel, avgMetrics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvVer = CrossValidatorVerbose(estimator = crossPipe, estimatorParamMaps = grid, evaluator= evaluator, numFolds = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = news_Title_Proc.randomSplit([0.7,0.3],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing models on fold 1\n",
      "params: {'elasticNetParam': 0.6, 'regParam': 1.0}\trmse: 148.102401\tavg: 148.102401\n",
      "params: {'elasticNetParam': 0.6, 'regParam': 1.5}\trmse: 146.493627\tavg: 146.493627\n",
      "params: {'elasticNetParam': 0.6, 'regParam': 2.0}\trmse: 145.602364\tavg: 145.602364\n",
      "params: {'elasticNetParam': 0.6, 'regParam': 2.5}\trmse: 144.895292\tavg: 144.895292\n",
      "params: {'elasticNetParam': 0.8, 'regParam': 1.0}\trmse: 146.929802\tavg: 146.929802\n",
      "params: {'elasticNetParam': 0.8, 'regParam': 1.5}\trmse: 145.640423\tavg: 145.640423\n",
      "params: {'elasticNetParam': 0.8, 'regParam': 2.0}\trmse: 144.700350\tavg: 144.700350\n",
      "params: {'elasticNetParam': 0.8, 'regParam': 2.5}\trmse: 143.918383\tavg: 143.918383\n",
      "params: {'elasticNetParam': 1.0, 'regParam': 1.0}\trmse: 146.242184\tavg: 146.242184\n",
      "params: {'elasticNetParam': 1.0, 'regParam': 1.5}\trmse: 144.958880\tavg: 144.958880\n",
      "params: {'elasticNetParam': 1.0, 'regParam': 2.0}\trmse: 143.933569\tavg: 143.933569\n",
      "params: {'elasticNetParam': 1.0, 'regParam': 2.5}\trmse: 143.331742\tavg: 143.331742\n",
      "Comparing models on fold 2\n",
      "params: {'elasticNetParam': 0.6, 'regParam': 1.0}\trmse: 168.852509\tavg: 158.477455\n",
      "params: {'elasticNetParam': 0.6, 'regParam': 1.5}\trmse: 166.801877\tavg: 156.647752\n",
      "params: {'elasticNetParam': 0.6, 'regParam': 2.0}\trmse: 165.744136\tavg: 155.673250\n",
      "params: {'elasticNetParam': 0.6, 'regParam': 2.5}\trmse: 165.112909\tavg: 155.004101\n",
      "params: {'elasticNetParam': 0.8, 'regParam': 1.0}\trmse: 167.710706\tavg: 157.320254\n",
      "params: {'elasticNetParam': 0.8, 'regParam': 1.5}\trmse: 165.799951\tavg: 155.720187\n",
      "params: {'elasticNetParam': 0.8, 'regParam': 2.0}\trmse: 164.949178\tavg: 154.824764\n",
      "params: {'elasticNetParam': 0.8, 'regParam': 2.5}\trmse: 164.362179\tavg: 154.140281\n",
      "params: {'elasticNetParam': 1.0, 'regParam': 1.0}\trmse: 166.490971\tavg: 156.366578\n",
      "params: {'elasticNetParam': 1.0, 'regParam': 1.5}\trmse: 165.216580\tavg: 155.087730\n",
      "params: {'elasticNetParam': 1.0, 'regParam': 2.0}\trmse: 164.406294\tavg: 154.169931\n",
      "params: {'elasticNetParam': 1.0, 'regParam': 2.5}\trmse: 163.790483\tavg: 153.561112\n",
      "Comparing models on fold 3\n",
      "params: {'elasticNetParam': 0.6, 'regParam': 1.0}\trmse: 163.184847\tavg: 160.046586\n",
      "params: {'elasticNetParam': 0.6, 'regParam': 1.5}\trmse: 162.017554\tavg: 158.437686\n",
      "params: {'elasticNetParam': 0.6, 'regParam': 2.0}\trmse: 161.412721\tavg: 157.586407\n",
      "params: {'elasticNetParam': 0.6, 'regParam': 2.5}\trmse: 160.992131\tavg: 157.000111\n",
      "params: {'elasticNetParam': 0.8, 'regParam': 1.0}\trmse: 162.353774\tavg: 158.998094\n",
      "params: {'elasticNetParam': 0.8, 'regParam': 1.5}\trmse: 161.442300\tavg: 157.627558\n",
      "params: {'elasticNetParam': 0.8, 'regParam': 2.0}\trmse: 160.904969\tavg: 156.851499\n",
      "params: {'elasticNetParam': 0.8, 'regParam': 2.5}\trmse: 160.534798\tavg: 156.271787\n",
      "params: {'elasticNetParam': 1.0, 'regParam': 1.0}\trmse: 161.798793\tavg: 158.177316\n",
      "params: {'elasticNetParam': 1.0, 'regParam': 1.5}\trmse: 161.046857\tavg: 157.074106\n",
      "params: {'elasticNetParam': 1.0, 'regParam': 2.0}\trmse: 160.553451\tavg: 156.297771\n",
      "params: {'elasticNetParam': 1.0, 'regParam': 2.5}\trmse: 160.131471\tavg: 155.751232\n",
      "Best model:\n",
      "params: {'elasticNetParam': 1.0, 'regParam': 2.5}\trmse: 155.751232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Date_Name: float, IDLink: double, Title: string, Headline: string, Source: string, Topic: string, Topic_vector: vector, SentimentTitle: double, SentimentHeadline: double, Facebook: bigint, GooglePlus: bigint, LinkedIn: bigint, Title_token: array<string>, Title_Stop: array<string>, Title_tf: vector, Title_tfidf: vector, features: vector, prediction: double]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "cvVer.fit(training).transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch Parameters automatically\n",
    "list1\n",
    "df_cross_val=pd.DataFrame(list1,columns=['Regularization_Parameters','RMSE','AVG'])\n",
    "df_cross_val.head()\n",
    "df_cross_val['Regularization_Parameters']=df_cross_val['Regularization_Parameters'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cross_val['Regularization_Parameters'] = df_cross_val['Regularization_Parameters'].replace({'regParam' : 'regP'}, regex=True)\n",
    "df_cross_val['Regularization_Parameters'] = df_cross_val['Regularization_Parameters'].replace({'elasticNetParam' : 'elNetP'}, regex=True)\n",
    "df_cross_val['Regularization_Parameters'] = df_cross_val['Regularization_Parameters'].replace({'{' : ''}, regex=True)\n",
    "df_cross_val['Regularization_Parameters'] = df_cross_val['Regularization_Parameters'].replace({'}' : ''}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Regularization_Parameters</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'elNetP': 0.6, 'regP': 1.0</td>\n",
       "      <td>148.102401</td>\n",
       "      <td>148.102401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'elNetP': 0.6, 'regP': 1.5</td>\n",
       "      <td>146.493627</td>\n",
       "      <td>146.493627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'elNetP': 0.6, 'regP': 2.0</td>\n",
       "      <td>145.602364</td>\n",
       "      <td>145.602364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'elNetP': 0.6, 'regP': 2.5</td>\n",
       "      <td>144.895292</td>\n",
       "      <td>144.895292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'elNetP': 0.8, 'regP': 1.0</td>\n",
       "      <td>146.929802</td>\n",
       "      <td>146.929802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'elNetP': 0.8, 'regP': 1.5</td>\n",
       "      <td>145.640423</td>\n",
       "      <td>145.640423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>'elNetP': 0.8, 'regP': 2.0</td>\n",
       "      <td>144.700350</td>\n",
       "      <td>144.700350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'elNetP': 0.8, 'regP': 2.5</td>\n",
       "      <td>143.918383</td>\n",
       "      <td>143.918383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>'elNetP': 1.0, 'regP': 1.0</td>\n",
       "      <td>146.242184</td>\n",
       "      <td>146.242184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'elNetP': 1.0, 'regP': 1.5</td>\n",
       "      <td>144.958880</td>\n",
       "      <td>144.958880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Regularization_Parameters        RMSE         AVG\n",
       "0  'elNetP': 0.6, 'regP': 1.0  148.102401  148.102401\n",
       "1  'elNetP': 0.6, 'regP': 1.5  146.493627  146.493627\n",
       "2  'elNetP': 0.6, 'regP': 2.0  145.602364  145.602364\n",
       "3  'elNetP': 0.6, 'regP': 2.5  144.895292  144.895292\n",
       "4  'elNetP': 0.8, 'regP': 1.0  146.929802  146.929802\n",
       "5  'elNetP': 0.8, 'regP': 1.5  145.640423  145.640423\n",
       "6  'elNetP': 0.8, 'regP': 2.0  144.700350  144.700350\n",
       "7  'elNetP': 0.8, 'regP': 2.5  143.918383  143.918383\n",
       "8  'elNetP': 1.0, 'regP': 1.0  146.242184  146.242184\n",
       "9  'elNetP': 1.0, 'regP': 1.5  144.958880  144.958880"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cross_val.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Regularization_Parameters</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>'elNetP': 1.0, 'regP': 2.5</td>\n",
       "      <td>143.331742</td>\n",
       "      <td>143.331742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'elNetP': 0.8, 'regP': 2.5</td>\n",
       "      <td>143.918383</td>\n",
       "      <td>143.918383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>'elNetP': 1.0, 'regP': 2.0</td>\n",
       "      <td>143.933569</td>\n",
       "      <td>143.933569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>'elNetP': 0.8, 'regP': 2.0</td>\n",
       "      <td>144.700350</td>\n",
       "      <td>144.700350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'elNetP': 0.6, 'regP': 2.5</td>\n",
       "      <td>144.895292</td>\n",
       "      <td>144.895292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Regularization_Parameters        RMSE         AVG\n",
       "11  'elNetP': 1.0, 'regP': 2.5  143.331742  143.331742\n",
       "7   'elNetP': 0.8, 'regP': 2.5  143.918383  143.918383\n",
       "10  'elNetP': 1.0, 'regP': 2.0  143.933569  143.933569\n",
       "6   'elNetP': 0.8, 'regP': 2.0  144.700350  144.700350\n",
       "3   'elNetP': 0.6, 'regP': 2.5  144.895292  144.895292"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cross_val_best=df_cross_val.nsmallest(10, 'RMSE')\n",
    "df_cross_val_best.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights derived from Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<center> <img src=\"Weights.PNG\" width=\"50%\" align=\"center\"> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Linear Regression with Title tfidf\n",
    "pipe_model = Pipeline(stages=[\n",
    "  feature.VectorAssembler(inputCols=['Title_tfidf'], outputCol='features'),\n",
    "  regression.LinearRegression(featuresCol='features', labelCol='LinkedIn')  \n",
    "]).fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|LinkedIn|         prediction|\n",
      "+--------+-------------------+\n",
      "|       2| -40.56827288461393|\n",
      "|      10|  8.621986764496882|\n",
      "|       0|   22.7774664105211|\n",
      "|       0| 13.915897851914515|\n",
      "|       5|-18.387598109956713|\n",
      "+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_df = pipe_model.transform(validation_df).select('LinkedIn','prediction')\n",
    "prediction_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_df = prediction_df.select((fn.col('prediction')- fn.col('LinkedIn')).alias('e')).\\\n",
    "    select(fn.sqrt(fn.avg(fn.col('e')**2)).alias(\"rmse\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              rmse|\n",
      "+------------------+\n",
      "|131.00310539927204|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rmse_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = tfidf_Title.stages[2].vocabulary\n",
    "vocabulary\n",
    "weights = pipe_model.stages[-1].coefficients.toArray()\n",
    "weights\n",
    "coeffs_df = pd.DataFrame({'word': vocabulary, 'weight': weights})\n",
    "\n",
    "negative_coeffs= coeffs_df.sort_values('weight',ascending=True).head(10)\n",
    "positive_coeffs = coeffs_df.sort_values('weight',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13931</th>\n",
       "      <td>plumbs</td>\n",
       "      <td>377.488826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8440</th>\n",
       "      <td>approaching</td>\n",
       "      <td>217.782003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20167</th>\n",
       "      <td>bests</td>\n",
       "      <td>200.231914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>linkedin</td>\n",
       "      <td>153.563009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24439</th>\n",
       "      <td>hurtful</td>\n",
       "      <td>152.550771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5619</th>\n",
       "      <td>queries</td>\n",
       "      <td>146.051192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25769</th>\n",
       "      <td>frightful</td>\n",
       "      <td>114.187867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8932</th>\n",
       "      <td>accounting</td>\n",
       "      <td>109.958851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3043</th>\n",
       "      <td>acquire</td>\n",
       "      <td>108.708127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14880</th>\n",
       "      <td>workflows</td>\n",
       "      <td>108.187819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              word      weight\n",
       "13931       plumbs  377.488826\n",
       "8440   approaching  217.782003\n",
       "20167        bests  200.231914\n",
       "233       linkedin  153.563009\n",
       "24439      hurtful  152.550771\n",
       "5619       queries  146.051192\n",
       "25769    frightful  114.187867\n",
       "8932    accounting  109.958851\n",
       "3043       acquire  108.708127\n",
       "14880    workflows  108.187819"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6971</th>\n",
       "      <td>findings</td>\n",
       "      <td>-151.376315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25228</th>\n",
       "      <td>dynamicpoint</td>\n",
       "      <td>-115.176959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18834</th>\n",
       "      <td>huffpo</td>\n",
       "      <td>-101.494627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4434</th>\n",
       "      <td>networking</td>\n",
       "      <td>-87.482485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21169</th>\n",
       "      <td>haggle</td>\n",
       "      <td>-83.781416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28192</th>\n",
       "      <td>dataset</td>\n",
       "      <td>-83.513982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11793</th>\n",
       "      <td>rubbish</td>\n",
       "      <td>-82.852576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24326</th>\n",
       "      <td>surround</td>\n",
       "      <td>-82.389163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12823</th>\n",
       "      <td>symantec</td>\n",
       "      <td>-80.962253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24903</th>\n",
       "      <td>spouts</td>\n",
       "      <td>-80.804652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               word      weight\n",
       "6971       findings -151.376315\n",
       "25228  dynamicpoint -115.176959\n",
       "18834        huffpo -101.494627\n",
       "4434     networking  -87.482485\n",
       "21169        haggle  -83.781416\n",
       "28192       dataset  -83.513982\n",
       "11793       rubbish  -82.852576\n",
       "24326      surround  -82.389163\n",
       "12823      symantec  -80.962253\n",
       "24903        spouts  -80.804652"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST REGRESSOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Following Models were executed using Random Forest\n",
    "     \n",
    "<br>\n",
    "<center> <img src=\"Random Forest Models.PNG\" width=\"50%\" align=\"center\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model 1 - Sentiment Title \n",
    "- Model 2 – Sentiment Title + Sentiment Headline\n",
    "- Model 3- Title(TFIDF) \n",
    "- Model 4 –TFIDF + Sentiment Title \n",
    "- Model 5 – Title(TFIDF) + Sentiment Title + Topic + Publish Date\n",
    "- Model 6 – (Word2Vec) Title + Sentiment Title +Publish Date + Topic\n",
    "- Model 7 – Cross Validation(depth=4, bins=5) (Word2Vec) Title + Sentiment Title +Publish Date + Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Model 1 - Random Forest using Sentiment Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Vector_Assembler = feature.VectorAssembler(inputCols=['SentimentTitle']\n",
    "                                           , outputCol='features')\n",
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4)\n",
    "\n",
    "rf = RandomForestRegressor(labelCol='LinkedIn',featuresCol=\"indexedFeatures\")\n",
    "pipeline = Pipeline(stages=[Vector_Assembler,featureIndexer, rf])\n",
    "model = pipeline.fit(training_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = model.transform(validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|LinkedIn|        prediction|\n",
      "+--------+------------------+\n",
      "|       2|19.227798887079537|\n",
      "|      10|28.521020634001047|\n",
      "|      12|17.464726886436146|\n",
      "|      55|14.576659042759378|\n",
      "|       0| 17.50035243634318|\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('LinkedIn','prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on validation data = 128.779\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"LinkedIn\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on validation data = %g\" % rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Model 2 - Random Forest using Sentiment Title and Sentiment Headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vector_Assembler = feature.VectorAssembler(inputCols=['SentimentTitle','SentimentHeadline']\n",
    "                                           , outputCol='features')\n",
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4)\n",
    "\n",
    "rf = RandomForestRegressor(labelCol='LinkedIn',featuresCol=\"indexedFeatures\")\n",
    "pipeline = Pipeline(stages=[Vector_Assembler,featureIndexer, rf])\n",
    "model = pipeline.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|LinkedIn|        prediction|\n",
      "+--------+------------------+\n",
      "|       2|19.343776963792152|\n",
      "|      10| 22.78334902674262|\n",
      "|      12| 18.08605653903623|\n",
      "|      55|14.388309185027733|\n",
      "|       0| 16.79581374381314|\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on validation data = 128.847\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(validation_df)\n",
    "predictions.select('LinkedIn','prediction').show(5)\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"LinkedIn\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on validation data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Model 3 - Title Column using TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_estimator = CountVectorizer(minTF=1., minDF=5).setInputCol('vector_no_stopw').setOutputCol('tf_Title')\n",
    "count_vectorizer_idf = IDF().setInputCol(\"tf_Title\").setOutputCol(\"tfidf_Title\")\n",
    "Vector_Assembler = feature.VectorAssembler(inputCols=['tfidf_Title']\n",
    "                                           , outputCol='features')\n",
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").setHandleInvalid(\"skip\")\n",
    "\n",
    "rf = RandomForestRegressor(labelCol='LinkedIn',featuresCol=\"indexedFeatures\")\n",
    "pipeline = Pipeline(stages=[count_vectorizer_estimator,count_vectorizer_idf,Vector_Assembler,featureIndexer, rf])\n",
    "model = pipeline.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|LinkedIn|        prediction|\n",
      "+--------+------------------+\n",
      "|       2|15.014408961346433|\n",
      "|       0|15.014408961346433|\n",
      "|       0|15.014408961346433|\n",
      "|       0|15.014408961346433|\n",
      "|      12|15.014408961346433|\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(validation_df)\n",
    "predictions.select('LinkedIn','prediction').show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|        prediction|count|\n",
      "+------------------+-----+\n",
      "| 16.55651313003063|    1|\n",
      "|25.806002264223878|   10|\n",
      "|14.189645019817643|    1|\n",
      "|173.83716666487186|    2|\n",
      "| 17.86705025555253|    2|\n",
      "| 536.2531276648439|    7|\n",
      "| 24.29103390870653|   14|\n",
      "| 14.23638982906312|    3|\n",
      "| 80.67221771743706|    1|\n",
      "|1034.9001121903593|    3|\n",
      "|220.87656214981158|    1|\n",
      "| 29.99996291094066|    4|\n",
      "|13.883443028059663|   17|\n",
      "|1025.5178018283486|    1|\n",
      "|28.040045896590357|    1|\n",
      "|17.906262762787822|    1|\n",
      "| 231.8742415975305|    3|\n",
      "| 252.8142662496235|    1|\n",
      "|27.189235300747562|   34|\n",
      "|103.56431693081515|    1|\n",
      "+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.groupby('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|           rmse|\n",
      "+---------------+\n",
      "|96.199555384443|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rmse_df = predictions.select((fn.col('prediction')- fn.col('LinkedIn')).alias('e')).\\\n",
    "    select(fn.sqrt(fn.avg(fn.col('e')**2)).alias(\"rmse\"))\n",
    "rmse_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Model 4 - Random Forest using Sentiment Title and Title ( TF-IDF )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_estimator = CountVectorizer(minTF=1., minDF=5).setInputCol('vector_no_stopw').setOutputCol('tf_Title')\n",
    "count_vectorizer_idf = IDF().setInputCol(\"tf_Title\").setOutputCol(\"tfidf_Title\")\n",
    "Vector_Assembler = feature.VectorAssembler(inputCols=['tfidf_Title','SentimentTitle']\n",
    "                                           , outputCol='features')\n",
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").setHandleInvalid(\"skip\")\n",
    "\n",
    "rf = RandomForestRegressor(labelCol='LinkedIn',featuresCol=\"indexedFeatures\")\n",
    "pipeline = Pipeline(stages=[count_vectorizer_estimator,count_vectorizer_idf,Vector_Assembler,featureIndexer, rf])\n",
    "model = pipeline.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|LinkedIn|        prediction|\n",
      "+--------+------------------+\n",
      "|       2|15.337750250341092|\n",
      "|       0|15.337750250341092|\n",
      "|       0|15.337750250341092|\n",
      "|      10|15.337750250341092|\n",
      "|       0|15.337750250341092|\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(validation_df)\n",
    "predictions.select('LinkedIn','prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              rmse|\n",
      "+------------------+\n",
      "|122.19902085157491|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rmse_df = predictions.select((fn.col('prediction')- fn.col('LinkedIn')).alias('e')).\\\n",
    "    select(fn.sqrt(fn.avg(fn.col('e')**2)).alias(\"rmse\"))\n",
    "rmse_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 5 – Title(TFIDF) + Sentiment Title + Topic + Publish Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_estimator = CountVectorizer(minTF=1., minDF=5).setInputCol('vector_no_stopw').setOutputCol('tf_Title')\n",
    "count_vectorizer_idf = IDF().setInputCol(\"tf_Title\").setOutputCol(\"tfidf_Title\")\n",
    "Vector_Assembler = feature.VectorAssembler(inputCols=['tfidf_Title','Topic_vector','Date_Name','SentimentTitle']\n",
    "                                           , outputCol='features')\n",
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").setHandleInvalid(\"skip\")\n",
    "\n",
    "rf = RandomForestRegressor(labelCol='LinkedIn',featuresCol=\"indexedFeatures\")\n",
    "pipeline = Pipeline(stages=[count_vectorizer_estimator,count_vectorizer_idf,Vector_Assembler,featureIndexer, rf])\n",
    "model = pipeline.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|LinkedIn|        prediction|\n",
      "+--------+------------------+\n",
      "|      10|12.815479772988045|\n",
      "|       0|12.815479772988045|\n",
      "|      12|12.815479772988045|\n",
      "|      10|12.815479772988045|\n",
      "|      55|12.815479772988045|\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(validation_df)\n",
    "predictions.select('LinkedIn','prediction').show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             rmse|\n",
      "+-----------------+\n",
      "|176.4165554760131|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rmse_df = predictions.select((fn.col('prediction')- fn.col('LinkedIn')).alias('e')).\\\n",
    "    select(fn.sqrt(fn.avg(fn.col('e')**2)).alias(\"rmse\"))\n",
    "rmse_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 6 – (Word2Vec) Title + Sentiment Title +Publish Date + Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vector_Assembler = feature.VectorAssembler(inputCols=['Title_Vect']\n",
    "                                           , outputCol='features')\n",
    "\n",
    "featureIndexer =\\\n",
    "    feature.VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").setHandleInvalid(\"skip\")\n",
    "\n",
    "rf = regression.RandomForestRegressor(labelCol='LinkedIn',featuresCol=\"indexedFeatures\")\n",
    "pipeline = Pipeline(stages=[Vector_Assembler,featureIndexer, rf])\n",
    "model = pipeline.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|LinkedIn|        prediction|\n",
      "+--------+------------------+\n",
      "|       2|12.783914780685615|\n",
      "|      10| 14.62001131473707|\n",
      "|       0| 28.05221125411119|\n",
      "|       0| 19.88761938278107|\n",
      "|       5|13.187948082097467|\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions= model.transform(validation_df)\n",
    "predictions.select('LinkedIn','prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             rmse|\n",
      "+-----------------+\n",
      "|92.87815713084893|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rmse_df = predictions.select((fn.col('prediction')- fn.col('LinkedIn')).alias('e')).\\\n",
    "    select(fn.sqrt(fn.avg(fn.col('e')**2)).alias(\"rmse\"))\n",
    "rmse_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 7 – Cross Validation(depth=4, bins=5) (Word2Vec) Title + Sentiment Title +Publish Date + Topic\n",
    "- This includes Random Forest using **Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: long (nullable = true)\n",
      " |-- Title_Vect: vector (nullable = true)\n",
      " |-- SentimentTitle: double (nullable = true)\n",
      " |-- Date_Name: float (nullable = true)\n",
      " |-- Topic_vector: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = ['Title_Vect','SentimentTitle','Date_Name','Topic_vector']  \n",
    "lr_data = news_Title_Proc.select(fn.col(\"LinkedIn\").alias(\"label\"), *features)  \n",
    "lr_data.printSchema()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vector_Assembler = feature.VectorAssembler(inputCols=features\n",
    "                                           , outputCol='features')\n",
    "\n",
    "rf = regression.RandomForestRegressor(labelCol='label',featuresCol=\"features\")\n",
    "pipeline_cv = Pipeline(stages=[Vector_Assembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder,CrossValidatorModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "estimatorParam = ParamGridBuilder() \\\n",
    ".addGrid(rf.maxDepth, [4, 6, 8]) \\\n",
    ".addGrid(rf.maxBins, [5, 10, 20, 40]) \\\n",
    ".addGrid(rf.impurity, [\"variance\"]) \\\n",
    ".build()\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "list1= list()\n",
    "class CrossValidatorVerbose(CrossValidator):\n",
    "  def _fit(self, dataset):\n",
    "        est = self.getOrDefault(self.estimator)\n",
    "        epm = self.getOrDefault(self.estimatorParamMaps)\n",
    "        numModels = len(epm)\n",
    "\n",
    "        eva = self.getOrDefault(self.evaluator)\n",
    "        metricName = eva.getMetricName()\n",
    "\n",
    "        nFolds = self.getOrDefault(self.numFolds)\n",
    "        seed = self.getOrDefault(self.seed)\n",
    "        h = 1.0 / nFolds\n",
    "\n",
    "        randCol = self.uid + \"_rand\"\n",
    "        df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
    "        metrics = [0.0] * numModels\n",
    "\n",
    "        for i in range(nFolds):\n",
    "            foldNum = i + 1\n",
    "            print(\"Comparing models on fold %d\" % foldNum)\n",
    "\n",
    "            validateLB = i * h\n",
    "            validateUB = (i + 1) * h\n",
    "            condition = (df[randCol] >= validateLB) & (df[randCol] < validateUB)\n",
    "            validation = df.filter(condition)\n",
    "            train = df.filter(~condition)\n",
    "\n",
    "            for j in range(numModels):\n",
    "                paramMap = epm[j]\n",
    "                model = est.fit(train, paramMap)\n",
    "                # TODO: duplicate evaluator to take extra params from input\n",
    "                metric = eva.evaluate(model.transform(validation, paramMap))\n",
    "                metrics[j] += metric\n",
    "\n",
    "                avgSoFar = metrics[j] / foldNum\n",
    "                print(\"params: %s\\t%s: %f\\tavg: %f\" % (\n",
    "                    {param.name: val for (param, val) in paramMap.items()},\n",
    "                    metricName, metric, avgSoFar))\n",
    "                list1.append([{param.name: val for (param, val) in paramMap.items()},metric,avgSoFar])\n",
    "              # paramsList.append([[{param.name: val for (param, val) in paramMap.items()},metricName,metric,avgSoFar]])\n",
    "\n",
    "        if eva.isLargerBetter():\n",
    "            bestIndex = np.argmax(metrics)\n",
    "        else:\n",
    "            bestIndex = np.argmin(metrics)\n",
    "\n",
    "        bestParams = epm[bestIndex]\n",
    "        bestModel = est.fit(dataset, bestParams)\n",
    "        avgMetrics = [m / nFolds for m in metrics]\n",
    "        bestAvg = avgMetrics[bestIndex]\n",
    "        print(\"Best model:\\nparams: %s\\t%s: %f\" % (\n",
    "            {param.name: val for (param, val) in bestParams.items()},\n",
    "            metricName, bestAvg))\n",
    "\n",
    "        return self._copyValues(CrossValidatorModel(bestModel, avgMetrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvVer = CrossValidatorVerbose(estimator = pipeline_cv, estimatorParamMaps = estimatorParam, evaluator= evaluator, numFolds = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,test_df = lr_data.randomSplit([0.8, 0.2], seed=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing models on fold 1\n",
      "params: {'maxDepth': 4, 'maxBins': 5, 'impurity': 'variance'}\trmse: 81.723454\tavg: 81.723454\n",
      "params: {'maxDepth': 4, 'maxBins': 10, 'impurity': 'variance'}\trmse: 82.621209\tavg: 82.621209\n",
      "params: {'maxDepth': 4, 'maxBins': 20, 'impurity': 'variance'}\trmse: 82.747279\tavg: 82.747279\n",
      "params: {'maxDepth': 4, 'maxBins': 40, 'impurity': 'variance'}\trmse: 83.215201\tavg: 83.215201\n",
      "params: {'maxDepth': 6, 'maxBins': 5, 'impurity': 'variance'}\trmse: 85.073498\tavg: 85.073498\n",
      "params: {'maxDepth': 6, 'maxBins': 10, 'impurity': 'variance'}\trmse: 85.339829\tavg: 85.339829\n",
      "params: {'maxDepth': 6, 'maxBins': 20, 'impurity': 'variance'}\trmse: 88.806712\tavg: 88.806712\n",
      "params: {'maxDepth': 6, 'maxBins': 40, 'impurity': 'variance'}\trmse: 90.707984\tavg: 90.707984\n",
      "params: {'maxDepth': 8, 'maxBins': 5, 'impurity': 'variance'}\trmse: 91.347478\tavg: 91.347478\n",
      "params: {'maxDepth': 8, 'maxBins': 10, 'impurity': 'variance'}\trmse: 87.239094\tavg: 87.239094\n",
      "params: {'maxDepth': 8, 'maxBins': 20, 'impurity': 'variance'}\trmse: 91.805105\tavg: 91.805105\n",
      "params: {'maxDepth': 8, 'maxBins': 40, 'impurity': 'variance'}\trmse: 93.215292\tavg: 93.215292\n",
      "Comparing models on fold 2\n",
      "params: {'maxDepth': 4, 'maxBins': 5, 'impurity': 'variance'}\trmse: 128.163901\tavg: 104.943677\n",
      "params: {'maxDepth': 4, 'maxBins': 10, 'impurity': 'variance'}\trmse: 127.507932\tavg: 105.064570\n",
      "params: {'maxDepth': 4, 'maxBins': 20, 'impurity': 'variance'}\trmse: 127.259788\tavg: 105.003534\n",
      "params: {'maxDepth': 4, 'maxBins': 40, 'impurity': 'variance'}\trmse: 127.660754\tavg: 105.437977\n",
      "params: {'maxDepth': 6, 'maxBins': 5, 'impurity': 'variance'}\trmse: 129.498033\tavg: 107.285766\n",
      "params: {'maxDepth': 6, 'maxBins': 10, 'impurity': 'variance'}\trmse: 128.836597\tavg: 107.088213\n",
      "params: {'maxDepth': 6, 'maxBins': 20, 'impurity': 'variance'}\trmse: 129.271763\tavg: 109.039238\n",
      "params: {'maxDepth': 6, 'maxBins': 40, 'impurity': 'variance'}\trmse: 128.246076\tavg: 109.477030\n",
      "params: {'maxDepth': 8, 'maxBins': 5, 'impurity': 'variance'}\trmse: 129.306798\tavg: 110.327138\n",
      "params: {'maxDepth': 8, 'maxBins': 10, 'impurity': 'variance'}\trmse: 129.090578\tavg: 108.164836\n",
      "params: {'maxDepth': 8, 'maxBins': 20, 'impurity': 'variance'}\trmse: 128.903214\tavg: 110.354160\n",
      "params: {'maxDepth': 8, 'maxBins': 40, 'impurity': 'variance'}\trmse: 128.155413\tavg: 110.685353\n",
      "Comparing models on fold 3\n",
      "params: {'maxDepth': 4, 'maxBins': 5, 'impurity': 'variance'}\trmse: 157.661489\tavg: 122.516281\n",
      "params: {'maxDepth': 4, 'maxBins': 10, 'impurity': 'variance'}\trmse: 157.848994\tavg: 122.659378\n",
      "params: {'maxDepth': 4, 'maxBins': 20, 'impurity': 'variance'}\trmse: 158.074111\tavg: 122.693726\n",
      "params: {'maxDepth': 4, 'maxBins': 40, 'impurity': 'variance'}\trmse: 158.308179\tavg: 123.061378\n",
      "params: {'maxDepth': 6, 'maxBins': 5, 'impurity': 'variance'}\trmse: 157.974811\tavg: 124.182114\n",
      "params: {'maxDepth': 6, 'maxBins': 10, 'impurity': 'variance'}\trmse: 159.409768\tavg: 124.528731\n",
      "params: {'maxDepth': 6, 'maxBins': 20, 'impurity': 'variance'}\trmse: 159.429036\tavg: 125.835837\n",
      "params: {'maxDepth': 6, 'maxBins': 40, 'impurity': 'variance'}\trmse: 160.267188\tavg: 126.407083\n",
      "params: {'maxDepth': 8, 'maxBins': 5, 'impurity': 'variance'}\trmse: 159.551552\tavg: 126.735276\n",
      "params: {'maxDepth': 8, 'maxBins': 10, 'impurity': 'variance'}\trmse: 159.784984\tavg: 125.371552\n",
      "params: {'maxDepth': 8, 'maxBins': 20, 'impurity': 'variance'}\trmse: 160.054151\tavg: 126.920824\n",
      "params: {'maxDepth': 8, 'maxBins': 40, 'impurity': 'variance'}\trmse: 161.357672\tavg: 127.576126\n",
      "Best model:\n",
      "params: {'maxDepth': 4, 'maxBins': 5, 'impurity': 'variance'}\trmse: 122.516281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: bigint, Title_Vect: vector, SentimentTitle: double, Date_Name: float, Topic_vector: vector, features: vector, prediction: double]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvVer.fit(train_df).transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cross_val=pd.DataFrame(list1,columns=['RF_Parameters','RMSE','AVG'])\n",
    "df_cross_val.head()\n",
    "df_cross_val['RF_Parameters']=df_cross_val['RF_Parameters'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cross_val['RF_Parameters'] = df_cross_val['RF_Parameters'].replace({'maxDepth' : 'Depth'}, regex=True)\n",
    "df_cross_val['RF_Parameters'] = df_cross_val['RF_Parameters'].replace({'maxBins' : 'Bins'}, regex=True)\n",
    "df_cross_val['RF_Parameters'] = df_cross_val['RF_Parameters'].replace({'{' : ''}, regex=True)\n",
    "df_cross_val['RF_Parameters'] = df_cross_val['RF_Parameters'].replace({'}' : ''}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RF_Parameters</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Depth': 4, 'Bins': 5, 'impurity': 'variance'</td>\n",
       "      <td>81.723454</td>\n",
       "      <td>81.723454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Depth': 4, 'Bins': 10, 'impurity': 'variance'</td>\n",
       "      <td>82.621209</td>\n",
       "      <td>82.621209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'Depth': 4, 'Bins': 20, 'impurity': 'variance'</td>\n",
       "      <td>82.747279</td>\n",
       "      <td>82.747279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'Depth': 4, 'Bins': 40, 'impurity': 'variance'</td>\n",
       "      <td>83.215201</td>\n",
       "      <td>83.215201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Depth': 6, 'Bins': 5, 'impurity': 'variance'</td>\n",
       "      <td>85.073498</td>\n",
       "      <td>85.073498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'Depth': 6, 'Bins': 10, 'impurity': 'variance'</td>\n",
       "      <td>85.339829</td>\n",
       "      <td>85.339829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>'Depth': 6, 'Bins': 20, 'impurity': 'variance'</td>\n",
       "      <td>88.806712</td>\n",
       "      <td>88.806712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'Depth': 6, 'Bins': 40, 'impurity': 'variance'</td>\n",
       "      <td>90.707984</td>\n",
       "      <td>90.707984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>'Depth': 8, 'Bins': 5, 'impurity': 'variance'</td>\n",
       "      <td>91.347478</td>\n",
       "      <td>91.347478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'Depth': 8, 'Bins': 10, 'impurity': 'variance'</td>\n",
       "      <td>87.239094</td>\n",
       "      <td>87.239094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    RF_Parameters       RMSE        AVG\n",
       "0   'Depth': 4, 'Bins': 5, 'impurity': 'variance'  81.723454  81.723454\n",
       "1  'Depth': 4, 'Bins': 10, 'impurity': 'variance'  82.621209  82.621209\n",
       "2  'Depth': 4, 'Bins': 20, 'impurity': 'variance'  82.747279  82.747279\n",
       "3  'Depth': 4, 'Bins': 40, 'impurity': 'variance'  83.215201  83.215201\n",
       "4   'Depth': 6, 'Bins': 5, 'impurity': 'variance'  85.073498  85.073498\n",
       "5  'Depth': 6, 'Bins': 10, 'impurity': 'variance'  85.339829  85.339829\n",
       "6  'Depth': 6, 'Bins': 20, 'impurity': 'variance'  88.806712  88.806712\n",
       "7  'Depth': 6, 'Bins': 40, 'impurity': 'variance'  90.707984  90.707984\n",
       "8   'Depth': 8, 'Bins': 5, 'impurity': 'variance'  91.347478  91.347478\n",
       "9  'Depth': 8, 'Bins': 10, 'impurity': 'variance'  87.239094  87.239094"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cross_val.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RF_Parameters</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Depth': 4, 'Bins': 5, 'impurity': 'variance'</td>\n",
       "      <td>81.723454</td>\n",
       "      <td>81.723454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Depth': 4, 'Bins': 10, 'impurity': 'variance'</td>\n",
       "      <td>82.621209</td>\n",
       "      <td>82.621209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'Depth': 4, 'Bins': 20, 'impurity': 'variance'</td>\n",
       "      <td>82.747279</td>\n",
       "      <td>82.747279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'Depth': 4, 'Bins': 40, 'impurity': 'variance'</td>\n",
       "      <td>83.215201</td>\n",
       "      <td>83.215201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Depth': 6, 'Bins': 5, 'impurity': 'variance'</td>\n",
       "      <td>85.073498</td>\n",
       "      <td>85.073498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    RF_Parameters       RMSE        AVG\n",
       "0   'Depth': 4, 'Bins': 5, 'impurity': 'variance'  81.723454  81.723454\n",
       "1  'Depth': 4, 'Bins': 10, 'impurity': 'variance'  82.621209  82.621209\n",
       "2  'Depth': 4, 'Bins': 20, 'impurity': 'variance'  82.747279  82.747279\n",
       "3  'Depth': 4, 'Bins': 40, 'impurity': 'variance'  83.215201  83.215201\n",
       "4   'Depth': 6, 'Bins': 5, 'impurity': 'variance'  85.073498  85.073498"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cross_val_best=df_cross_val.nsmallest(10, 'RMSE')\n",
    "df_cross_val_best.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBT Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Following Models were executed using GBT Regressor\n",
    "     \n",
    "<br>\n",
    "<center> <img src=\"GBT Regressor.PNG\" width=\"50%\" align=\"center\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model 1-Title(TFIDF) + Sentiment Title\n",
    "- Model 2 - Title (TFIDF) + Sentiment Title + Topic(One Hot)\n",
    "- Model 3- Title (TFIDF)\n",
    "- Model 4 – Title (TFIDF) +Sentiment Title + Topic (One Hot) + Publish date\n",
    "- Model 5 – Title(Word2Vect) +Sentiment Title + Topic (One Hot) + Publish date "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1-Title(TFIDF) + Sentiment Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_estimator = CountVectorizer(minTF=1., minDF=5).setInputCol('vector_no_stopw').setOutputCol('tf_Title')\n",
    "count_vectorizer_idf = IDF().setInputCol(\"tf_Title\").setOutputCol(\"tfidf_Title\")\n",
    "Vector_Assembler = feature.VectorAssembler(inputCols=['tfidf_Title','SentimentTitle']\n",
    "                                           , outputCol='features')\n",
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").setHandleInvalid(\"skip\")\n",
    "\n",
    "gbt = GBTRegressor(labelCol='LinkedIn',featuresCol=\"indexedFeatures\")\n",
    "pipeline = Pipeline(stages=[count_vectorizer_estimator,count_vectorizer_idf,Vector_Assembler,featureIndexer, gbt])\n",
    "model = pipeline.fit(training_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|LinkedIn|        prediction|\n",
      "+--------+------------------+\n",
      "|       2|11.092685355244646|\n",
      "|       0|12.202847466714859|\n",
      "|      10|12.202847466714859|\n",
      "|     129|12.202847466714859|\n",
      "|       0|12.202847466714859|\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(validation_df)\n",
    "predictions.select('LinkedIn','prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              rmse|\n",
      "+------------------+\n",
      "|149.81984260843709|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rmse_df = predictions.select((fn.col('prediction')- fn.col('LinkedIn')).alias('e')).\\\n",
    "    select(fn.sqrt(fn.avg(fn.col('e')**2)).alias(\"rmse\"))\n",
    "rmse_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 - Title (TFIDF) + Sentiment Title + Topic(One Hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_estimator = CountVectorizer(minTF=1., minDF=5).setInputCol('vector_no_stopw').setOutputCol('tf_Title')\n",
    "count_vectorizer_idf = IDF().setInputCol(\"tf_Title\").setOutputCol(\"tfidf_Title\")\n",
    "Vector_Assembler = feature.VectorAssembler(inputCols=['tfidf_Title','SentimentTitle','Topic_vector']\n",
    "                                           , outputCol='features')\n",
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").setHandleInvalid(\"skip\")\n",
    "\n",
    "gbt = GBTRegressor(labelCol='LinkedIn',featuresCol=\"indexedFeatures\")\n",
    "pipeline = Pipeline(stages=[count_vectorizer_estimator,count_vectorizer_idf,Vector_Assembler,featureIndexer, gbt])\n",
    "model = pipeline.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|LinkedIn|        prediction|\n",
      "+--------+------------------+\n",
      "|       0|12.035336330541108|\n",
      "|       0|23.956569204524165|\n",
      "|      12|12.035336330541108|\n",
      "|       4|12.035336330541108|\n",
      "|       0|12.035336330541108|\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(validation_df)\n",
    "predictions.select('LinkedIn','prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              rmse|\n",
      "+------------------+\n",
      "|129.05745073120403|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rmse_df = predictions.select((fn.col('prediction')- fn.col('LinkedIn')).alias('e')).\\\n",
    "    select(fn.sqrt(fn.avg(fn.col('e')**2)).alias(\"rmse\"))\n",
    "rmse_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3- Title (TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_estimator = CountVectorizer(minTF=1., minDF=5).setInputCol('vector_no_stopw').setOutputCol('tf_Title')\n",
    "count_vectorizer_idf = IDF().setInputCol(\"tf_Title\").setOutputCol(\"tfidf_Title\")\n",
    "Vector_Assembler = feature.VectorAssembler(inputCols=['tfidf_Title']\n",
    "                                           , outputCol='features')\n",
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").setHandleInvalid(\"skip\")\n",
    "\n",
    "gbt = GBTRegressor(labelCol='LinkedIn',featuresCol=\"indexedFeatures\")\n",
    "pipeline = Pipeline(stages=[count_vectorizer_estimator,count_vectorizer_idf,Vector_Assembler,featureIndexer, gbt])\n",
    "model = pipeline.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|LinkedIn|        prediction|\n",
      "+--------+------------------+\n",
      "|       0|11.857722340539421|\n",
      "|       0|11.857722340539421|\n",
      "|     129|11.857722340539421|\n",
      "|      46|11.857722340539421|\n",
      "|       0|11.857722340539421|\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(validation_df)\n",
    "predictions.select('LinkedIn','prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             rmse|\n",
      "+-----------------+\n",
      "|122.5777926633404|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rmse_df = predictions.select((fn.col('prediction')- fn.col('LinkedIn')).alias('e')).\\\n",
    "    select(fn.sqrt(fn.avg(fn.col('e')**2)).alias(\"rmse\"))\n",
    "rmse_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4 – Title (TFIDF) +Sentiment Title + Topic (One Hot) + Publish date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_estimator = CountVectorizer(minTF=1., minDF=5).setInputCol('vector_no_stopw').setOutputCol('tf_Title')\n",
    "count_vectorizer_idf = IDF().setInputCol(\"tf_Title\").setOutputCol(\"tfidf_Title\")\n",
    "Vector_Assembler = feature.VectorAssembler(inputCols=['tfidf_Title','Topic_vector','Date_Name','SentimentTitle']\n",
    "                                           , outputCol='features')\n",
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").setHandleInvalid(\"skip\")\n",
    "\n",
    "gbt = GBTRegressor(labelCol='LinkedIn',featuresCol=\"indexedFeatures\")\n",
    "pipeline = Pipeline(stages=[count_vectorizer_estimator,count_vectorizer_idf,Vector_Assembler,featureIndexer, gbt])\n",
    "model = pipeline.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|LinkedIn|        prediction|\n",
      "+--------+------------------+\n",
      "|       0|13.063043913033296|\n",
      "|      46|13.063043913033296|\n",
      "|      39|13.063043913033296|\n",
      "|      12|13.063043913033296|\n",
      "|       0|13.063043913033296|\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(validation_df)\n",
    "predictions.select('LinkedIn','prediction').show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              rmse|\n",
      "+------------------+\n",
      "|101.56705564184892|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rmse_df = predictions.select((fn.col('prediction')- fn.col('LinkedIn')).alias('e')).\\\n",
    "    select(fn.sqrt(fn.avg(fn.col('e')**2)).alias(\"rmse\"))\n",
    "rmse_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 5 – Title(Word2Vect) +Sentiment Title + Topic (One Hot) + Publish date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting Regression\n",
    "Vector_Assembler = feature.VectorAssembler(inputCols=['Title_Vect','SentimentTitle','Date_Name','Topic_vector']\n",
    "                                           , outputCol='features')\n",
    "featureIndexer =\\\n",
    "   feature.VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").setHandleInvalid(\"skip\")\n",
    "GBT_model = Pipeline(stages=[\n",
    "Vector_Assembler,featureIndexer,\n",
    "regression.GBTRegressor(featuresCol='indexedFeatures', labelCol='LinkedIn')]).fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|LinkedIn|        prediction|\n",
      "+--------+------------------+\n",
      "|       2| 29.62197438318668|\n",
      "|      10|10.359271496097856|\n",
      "|       0| 21.60001232652279|\n",
      "|       0| 16.50451981191586|\n",
      "|       5|13.241034403553702|\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions= GBT_model.transform(validation_df)\n",
    "predictions.select('LinkedIn','prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              rmse|\n",
      "+------------------+\n",
      "|167.59511130442692|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rmse_df = predictions.select((fn.col('prediction')- fn.col('LinkedIn')).alias('e')).\\\n",
    "    select(fn.sqrt(fn.avg(fn.col('e')**2)).alias(\"rmse\"))\n",
    "rmse_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import feature, regression, evaluation, Pipeline\n",
    "from pyspark.sql import functions as fn, Row\n",
    "import matplotlib.pyplot as plt\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Functionality for computing features\n",
    "from pyspark.ml import feature\n",
    "# Functionality for regression\n",
    "from pyspark.ml import regression\n",
    "# Funcionality for classification\n",
    "from pyspark.ml import classification\n",
    "# Object for creating sequences of transformations\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "datasource = pd.read_csv('News_Final.csv')\n",
    "\n",
    "datasource = datasource[datasource['LinkedIn'] != -1]\n",
    "\n",
    "datasource['Date_Name'] = pd.to_datetime(datasource[\"PublishDate\"]).dt.strftime(\"%Y%m%d\")\n",
    "datasource[['Title', 'Headline','Source','Topic','PublishDate','Date_Name']] = datasource[['Title', 'Headline','Source','Topic','PublishDate','Date_Name']].astype(str)\n",
    "datasource.columns = ['IDLInk', 'Title', 'Headline', 'Source', 'Topic', 'PublishDate', 'SentimentTitle', 'SentimentHeadline', 'Facebook', 'GooglePlus', 'LinkedIn','Date_Name']\n",
    "#datasource.columns\n",
    "news_df = spark.createDataFrame(datasource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_lower_df = news_df.select('IDLink',fn.lower(fn.col('Title')).alias('Title'),fn.lower(fn.col('Headline')).alias('Headline'),'Source',\n",
    "                               'Topic','PublishDate', 'SentimentTitle', 'SentimentHeadline', 'Facebook', 'GooglePlus', 'LinkedIn','Date_Name')\n",
    "                               \n",
    "\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, split\n",
    "\n",
    "# CLEANING TEXT STRING\n",
    "\n",
    "def clean_text(c):\n",
    "  c = lower(c)\n",
    "  c = regexp_replace(c, \"^rt \", \"\")\n",
    "  c = regexp_replace(c, \"(https?\\://)\\S+\", \"\")\n",
    "  c = regexp_replace(c, \"[^a-zA-Z0-9\\\\s]\", \"\")\n",
    "  c = regexp_replace(c,\"[\\d]+\",\"\")  \n",
    "  #c = split(c, \"\\\\s+\") tokenization...\n",
    "  return c\n",
    "\n",
    "clean_news_df = news_lower_df.withColumn('Title_Cleaned',clean_text(col('Title')))\n",
    "\n",
    "#TOKENIZER\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"Title_Cleaned\", outputCol=\"vector\")\n",
    "title_vector_df = tokenizer.transform(clean_news_df)\n",
    "\n",
    "#STOP WORD REMOVAL\n",
    "\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# Define a list of stop words or use default list\n",
    "remover = StopWordsRemover()\n",
    "stopwords = remover.getStopWords() \n",
    "\n",
    "# Specify input/output columns\n",
    "remover.setInputCol(\"vector\")\n",
    "remover.setOutputCol(\"vector_no_stopw\")\n",
    "\n",
    "# Transform existing dataframe with the StopWordsRemover\n",
    "Titlevector_no_stopw_df = remover.transform(title_vector_df)\n",
    "\n",
    "\n",
    "#BIGRAMS\n",
    "\n",
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "# Define NGram transformer\n",
    "ngram = NGram(n=2, inputCol=\"vector_no_stopw\", outputCol=\"bigrams\")\n",
    "\n",
    "# Create bigram_df as a transform of unigram_df using NGram tranformer\n",
    "production_df = ngram.transform(Titlevector_no_stopw_df)\n",
    "\n",
    "from pyspark.sql.functions import col, size\n",
    "\n",
    "production_df = production_df.where(size(col(\"bigrams\")) >= 2)\n",
    "\n",
    "#ALTERING THE TIME STAMP\n",
    "\n",
    "production_df2 = production_df.withColumn('Date_Name',production_df[\"Date_Name\"].cast(\"float\") )\n",
    "\n",
    "\n",
    "#ONE HOT ENCODING\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Topic\", outputCol=\"Topic_numeric\").fit(production_df2)\n",
    "indexed_df = indexer.transform(production_df2)\n",
    "\n",
    "#type(indexed_df[\"Topic_numeric\"])\n",
    "\n",
    "\n",
    "encoder = OneHotEncoderEstimator(\n",
    "    inputCols=[\"Topic_numeric\"],  \n",
    "    outputCols=[\"Topic_vector\"]\n",
    ")\n",
    "\n",
    "\n",
    "model = encoder.fit(indexed_df)\n",
    "\n",
    "production_df2 = model.transform(indexed_df)\n",
    "\n",
    "Title_Word2Vec = feature.Word2Vec(vectorSize=100, minCount=10, inputCol=\"vector_no_stopw\", outputCol=\"Title_Vect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df, validation_df = production_df2.randomSplit([0.8, 0.2], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+-------------+-------+-------------------+-------------------+------------------+--------+----------+--------+-----------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+\n",
      "|IDLink|               Title|            Headline|       Source|  Topic|        PublishDate|     SentimentTitle| SentimentHeadline|Facebook|GooglePlus|LinkedIn|  Date_Name|       Title_Cleaned|              vector|     vector_no_stopw|             bigrams|Topic_numeric| Topic_vector|\n",
      "+------+--------------------+--------------------+-------------+-------+-------------------+-------------------+------------------+--------+----------+--------+-----------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+\n",
      "|   2.0|to one ceo, 'jane...|\"\"\"i believe some...|  MarketWatch|economy|2015-11-09 22:40:10| 0.0376889180722205|-0.177667263629675|      42|         0|       0|2.0151108E7|to one ceo janet ...|[to, one, ceo, ja...|[one, ceo, janet,...|[one ceo, ceo jan...|          0.0|(3,[0],[1.0])|\n",
      "|   3.0|3 ways to jumpsta...|this article is p...|YES! Magazine|economy|2015-11-09 09:40:10|-0.0944911182523068| -0.06081670409998|      98|        23|       0|2.0151108E7| ways to jumpstar...|[, ways, to, jump...|[, ways, jumpstar...|[ ways, ways jump...|          0.0|(3,[0],[1.0])|\n",
      "+------+--------------------+--------------------+-------------+-------+-------------------+-------------------+------------------+--------+----------+--------+-----------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With all features\n",
    "Vector_Assembler = feature.VectorAssembler(inputCols=['Title_Vect','SentimentTitle','Date_Name','Topic_vector']\n",
    "                                           , outputCol='features')\n",
    "featureIndexer =\\\n",
    "    feature.VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").setHandleInvalid(\"skip\")\n",
    "\n",
    "rf = regression.RandomForestRegressor(labelCol='LinkedIn',featuresCol=\"features\", maxDepth=4, maxBins=5, impurity='variance')\n",
    "pipeline = Pipeline(stages=[Title_Word2Vec,Vector_Assembler,featureIndexer, rf])\n",
    "model = pipeline.fit(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GETTING INPUT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Row(Title='Obama might win the next elections', SentimentTitle=-0.451, Topic='Obama',PublishDate='20160801'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = [Row(Title='Donald Trump fuck stormy daniels', SentimentTitle=-0.451, Topic='Obama',PublishDate='20160801'),\n",
    "            Row(Title='Palestine is a serious issue', SentimentTitle=-0.22, Topic='Palestine',PublishDate='20170101'),\n",
    "            Row(Title='Economy is booming right now', SentimentTitle=0.65, Topic='Economy',PublishDate='20161111'),\n",
    "            Row(Title='Microsoft Conquered GitHub', SentimentTitle=0.75, Topic='Microsoft',PublishDate='20180801')\n",
    "           ]\n",
    "testing_df = spark.createDataFrame(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PublishDate: string (nullable = true)\n",
      " |-- SentimentTitle: double (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Topic: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testing_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------------+---------+\n",
      "|PublishDate|SentimentTitle|               Title|    Topic|\n",
      "+-----------+--------------+--------------------+---------+\n",
      "|   20160801|        -0.451|Donald Trump fuck...|    Obama|\n",
      "|   20170101|         -0.22|Palestine is a se...|Palestine|\n",
      "|   20161111|          0.65|Economy is boomin...|  Economy|\n",
      "|   20180801|          0.75|Microsoft Conquer...|Microsoft|\n",
      "+-----------+--------------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testing_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df = testing_df.withColumn('Date_Name',testing_df[\"PublishDate\"].cast(\"float\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PublishDate', 'SentimentTitle', 'Title', 'Topic', 'Date_Name']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df = testing_df.select(fn.lower(fn.col('PublishDate')).alias('PublishDate'),fn.lower(fn.col('Title')).alias('Title'),\n",
    "                               fn.lower(fn.col('Topic')).alias('Topic'),'SentimentTitle','Date_Name')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+---------+--------------+-----------+\n",
      "|PublishDate|               Title|    Topic|SentimentTitle|  Date_Name|\n",
      "+-----------+--------------------+---------+--------------+-----------+\n",
      "|   20160801|donald trump fuck...|    obama|        -0.451|  2.01608E7|\n",
      "|   20170101|palestine is a se...|palestine|         -0.22|  2.01701E7|\n",
      "|   20161111|economy is boomin...|  economy|          0.65|2.0161112E7|\n",
      "|   20180801|microsoft conquer...|microsoft|          0.75|  2.01808E7|\n",
      "+-----------+--------------------+---------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testing_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING TEXT STRING\n",
    "\n",
    "def clean_text(c):\n",
    "  c = lower(c)\n",
    "  c = regexp_replace(c, \"^rt \", \"\")\n",
    "  c = regexp_replace(c, \"(https?\\://)\\S+\", \"\")\n",
    "  c = regexp_replace(c, \"[^a-zA-Z0-9\\\\s]\", \"\")\n",
    "  c = regexp_replace(c,\"[\\d]+\",\"\")  \n",
    "  #c = split(c, \"\\\\s+\") tokenization...\n",
    "  return c\n",
    "\n",
    "testing_df = testing_df.withColumn('Title_Cleaned',clean_text(col('Title')))\n",
    "\n",
    "\n",
    "#TOKENIZER\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"Title_Cleaned\", outputCol=\"vector\")\n",
    "testing_df = tokenizer.transform(testing_df)\n",
    "\n",
    "#STOP WORD REMOVAL\n",
    "remover = StopWordsRemover()\n",
    "stopwords = remover.getStopWords() \n",
    "\n",
    "# Specify input/output columns\n",
    "remover.setInputCol(\"vector\")\n",
    "remover.setOutputCol(\"vector_no_stopw\")\n",
    "\n",
    "# Transform existing dataframe with the StopWordsRemover\n",
    "testing_df = remover.transform(testing_df)\n",
    "\n",
    "\n",
    "#BIGRAMS\n",
    "\n",
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "# Define NGram transformer\n",
    "ngram = NGram(n=2, inputCol=\"vector_no_stopw\", outputCol=\"bigrams\")\n",
    "\n",
    "# Create bigram_df as a transform of unigram_df using NGram tranformer\n",
    "testing_df = ngram.transform(testing_df)\n",
    "\n",
    "from pyspark.sql.functions import col, size\n",
    "\n",
    "#testing_df = testing_df.where(size(col(\"bigrams\")) >= 2)\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Topic\", outputCol=\"Topic_numeric\").fit(testing_df)\n",
    "indexed_df = indexer.transform(testing_df)\n",
    "\n",
    "#type(indexed_df[\"Topic_numeric\"])\n",
    "\n",
    "\n",
    "encoder = OneHotEncoderEstimator(\n",
    "    inputCols=[\"Topic_numeric\"],  \n",
    "    outputCols=[\"Topic_vector\"]\n",
    ")\n",
    "\n",
    "\n",
    "model_index = encoder.fit(indexed_df)\n",
    "\n",
    "testing_df = model_index.transform(indexed_df)\n",
    "\n",
    "Title_Word2Vec = feature.Word2Vec(vectorSize=100, minCount=10, inputCol=\"vector_no_stopw\", outputCol=\"Title_Vect\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+---------+--------------+-----------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+\n",
      "|PublishDate|               Title|    Topic|SentimentTitle|  Date_Name|       Title_Cleaned|              vector|     vector_no_stopw|             bigrams|Topic_numeric| Topic_vector|\n",
      "+-----------+--------------------+---------+--------------+-----------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+\n",
      "|   20160801|donald trump fuck...|    obama|        -0.451|  2.01608E7|donald trump fuck...|[donald, trump, f...|[donald, trump, f...|[donald trump, tr...|          0.0|(3,[0],[1.0])|\n",
      "|   20170101|palestine is a se...|palestine|         -0.22|  2.01701E7|palestine is a se...|[palestine, is, a...|[palestine, serio...|[palestine seriou...|          1.0|(3,[1],[1.0])|\n",
      "|   20161111|economy is boomin...|  economy|          0.65|2.0161112E7|economy is boomin...|[economy, is, boo...|[economy, booming...|[economy booming,...|          2.0|(3,[2],[1.0])|\n",
      "|   20180801|microsoft conquer...|microsoft|          0.75|  2.01808E7|microsoft conquer...|[microsoft, conqu...|[microsoft, conqu...|[microsoft conque...|          3.0|    (3,[],[])|\n",
      "+-----------+--------------------+---------+--------------+-----------+--------------------+--------------------+--------------------+--------------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testing_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|               Title|        prediction|\n",
      "+--------------------+------------------+\n",
      "|donald trump fuck...|14.697249327751198|\n",
      "|palestine is a se...| 5.390371327529724|\n",
      "|economy is boomin...|199.30676798164237|\n",
      "|microsoft conquer...| 78.87656860148579|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(testing_df).select('Title','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IDLink',\n",
       " 'Title',\n",
       " 'Headline',\n",
       " 'Source',\n",
       " 'Topic',\n",
       " 'PublishDate',\n",
       " 'SentimentTitle',\n",
       " 'SentimentHeadline',\n",
       " 'Facebook',\n",
       " 'GooglePlus',\n",
       " 'LinkedIn',\n",
       " 'Date_Name',\n",
       " 'Title_Cleaned',\n",
       " 'vector',\n",
       " 'vector_no_stopw',\n",
       " 'bigrams',\n",
       " 'Topic_numeric',\n",
       " 'Topic_vector']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "production_df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+--------------------+------+---------+-------------------+--------------+--------------------+--------+----------+--------+-----------+-------------+---------------+---------------+---------------+-------------+------------+\n",
      "| IDLink|              Title|            Headline|Source|    Topic|        PublishDate|SentimentTitle|   SentimentHeadline|Facebook|GooglePlus|LinkedIn|  Date_Name|Title_Cleaned|         vector|vector_no_stopw|        bigrams|Topic_numeric|Topic_vector|\n",
      "+-------+-------------------+--------------------+------+---------+-------------------+--------------+--------------------+--------+----------+--------+-----------+-------------+---------------+---------------+---------------+-------------+------------+\n",
      "|80690.0|monday, 29 feb 2016|ramallah, februar...|   nan|palestine|2016-02-28 14:03:00|           0.0|-0.00590569489076...|       0|         0|       0|2.0160228E7| monday  feb |[monday, , feb]|[monday, , feb]|[monday ,  feb]|          3.0|   (3,[],[])|\n",
      "|81052.0|monday, 29 feb 2016|ramallah, februar...|   nan|palestine|2016-03-01 09:29:00|           0.0|  0.0485459183673469|       0|         0|       0|  2.01603E7| monday  feb |[monday, , feb]|[monday, , feb]|[monday ,  feb]|          3.0|   (3,[],[])|\n",
      "+-------+-------------------+--------------------+------+---------+-------------------+--------------+--------------------+--------+----------+--------+-----------+-------------+---------------+---------------+---------------+-------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "production_df2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|LinkedIn|        prediction|\n",
      "+--------+------------------+\n",
      "|       2|14.108441937680325|\n",
      "|      10| 14.04221663826122|\n",
      "|     129|13.556403099633673|\n",
      "|      49|25.319727637287674|\n",
      "|       3|12.265480549261557|\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+\n",
      "|             rmse|\n",
      "+-----------------+\n",
      "|92.06034532259707|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_df = model.transform(validation_df).select('LinkedIn','prediction')\n",
    "prediction_df.show(5)\n",
    "rmse_df = prediction_df.select((fn.col('prediction')- fn.col('LinkedIn')).alias('e')).\\\n",
    "    select(fn.sqrt(fn.avg(fn.col('e')**2)).alias(\"rmse\"))\n",
    "\n",
    "rmse_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Our best model is Random Forest Regression on Sentiment Title, Topic ( One hot encoding ), Title( Word 2 Vec ) and Date**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A person could use our platform to input a Title and in return it will\n",
    "analysis and forecast/predict the popularity of that particular Title based\n",
    "on inputs with an best of RMSE of 81.72. Further the model could be\n",
    "improvised by providing more data i.e. data collected over longer period of\n",
    "time and data spread over more topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Source - UCI machine learning repository\n",
    "- Paper - Moniz, N., Torgo, L (2018). Multi-Source Social Feedback of Online News Feeds. Retrieved from\n",
    "https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
